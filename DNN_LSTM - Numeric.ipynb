{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b09e77",
   "metadata": {
    "id": "c0ad506d"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import textblob, string\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import layers, models, optimizers, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad41e68",
   "metadata": {
    "id": "In_zYnx7iyjm"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv('L:/Thesis/Dateset/FeaturesExtraction_Compeleted.csv', encoding='latin-1')\n",
    "\n",
    "df = df.astype(dtype={'title': 'string','url': 'string',\n",
    "                                'date_published': 'string','author_name': 'string',\n",
    "                                'content': 'string','label': 'string'})\n",
    "\n",
    "cols = ['countOfPosWord', 'countOfNegWord', 'NumberOfChar','NumberOfWords','NumberOfSentences','AvgCharPerWord','AvgWordPerSent','NumberOfUpCase','NumberOfPunctuatuion','Orgtitle_contentSim','TopicModellingSim','afinn_score','vader_score_compound','FREI','FKGL','ARI','GFI','CLI']\n",
    "data = pd.read_csv('L:/Thesis/Dateset/FeaturesExtraction_Compeleted.csv',  usecols = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d4d679b",
   "metadata": {
    "id": "qPaG74F9VoFX"
   },
   "outputs": [],
   "source": [
    "df['text_features']= df['title'] +' '+ df['content'] +' '+ df['author_name'] +' '+ df['date_published']\n",
    "df['Complete_Data'] = df['text_features'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7dc3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FALSE           9576\n",
       "TRUE            9559\n",
       "half-true       4248\n",
       "mostly-false    3437\n",
       "mostly-true     3196\n",
       "Name: label, dtype: Int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f5963f",
   "metadata": {
    "id": "6CXPMvsK2Iif"
   },
   "source": [
    "## **SpellCheker**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87d44f",
   "metadata": {
    "id": "j4HlFVjdHcP7"
   },
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d905b34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576291bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHWMrBrJbsg1",
    "outputId": "5ee1ab7f-43f4-4c5f-b020-1b71b8646bea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1091)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1091)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4cd58e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUyk98dgFhvi",
    "outputId": "17f1c138-ec57-4b63-f89a-41c482feaa21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for row in range(len(df)):\n",
    "  df['Complete_Data'][row] = re.sub(r\"won\\'t\", \"will not\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"can\\'t\", \"can not\", df['Complete_Data'][row])\n",
    "\n",
    "    # general\n",
    "  df['Complete_Data'][row] = re.sub(r\"n\\'t\", \" not\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'re\", \" are\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'s\", \" is\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'d\", \" would\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'ll\", \" will\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'t\", \" not\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'ve\", \" have\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'m\", \" am\", df['Complete_Data'][row])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0290285e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "592f0f46",
    "outputId": "3d66939c-728a-4d34-ab7b-9ea973ae71c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "X = df['Complete_Data']\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "import re\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "    document= [word for word in document if word not in stop_words]\n",
    "    document = [ps.stem(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "    \n",
    "df['Complete_Data'] = documents\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd16493",
   "metadata": {
    "id": "JhOwiMYbI2-a"
   },
   "source": [
    "**Add Numeric features to text features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fca9e9a9",
   "metadata": {
    "id": "dSQIHcNHdlML"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "data_scaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "957186cc",
   "metadata": {
    "id": "YHKQuov7dH_h"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "data_scaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb423681",
   "metadata": {
    "id": "kjR7uLY6dlTp"
   },
   "outputs": [],
   "source": [
    "numerical_data=list()\n",
    "for row in range(0,len(df)):\n",
    "  numerical_data.insert(row,data_scaled[row])\n",
    "df['numerical_features'] = numerical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da058a6f",
   "metadata": {
    "id": "4lDuf-IydlZ4"
   },
   "outputs": [],
   "source": [
    "df = df.astype(dtype={'numerical_features': 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c231eb2b",
   "metadata": {
    "id": "r8h8X_lzly67"
   },
   "outputs": [],
   "source": [
    "df['Complete_Data'] = df['Complete_Data'] + df['numerical_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abbd7256",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "bUbudSpF2UR9",
    "outputId": "80139565-ad55-4029-fa11-dc38489b6be0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â everi singl swing seat hous democrat endors medicareforal elect track win elect everi singl one â â unexpect elect loss hous democrat made headlin intern divis parti appeal voter â support progress polici lead member lose seat progress say â rep alexandria ocasio cortez democrat repres part bronx queen said anâ â hous democrat swing district support medicar elect without except â everi singl swing seat hous democrat endors medicareforal elect track win elect everi singl one â everi singl swing seat hous democrat endors elect track win elect everi ð ð ½ singl ð ð ½ one ð ð ½ claim gain traction twitter reader ask us look â gener swing seat competit hous district swing democrat republican control differ organ may rate differ seat competit cook polit report nonpartisan polit analysi organ highli regard commonli cite sourc final nov 2 cook list 23 swing seat held democrat incumb seek elect toss lean democrat 23 two incumb rep matt cartwright pennsylvania peter defazio oregonâ co sponsor provid medicar two â ocasio cortezâ campaign ask forâ evid claim provid longer list hous democrat said competit race list includ rep mike levin kati porter california susan wild pennsylvania ann kirkpatrick arizona co sponsor medicar act 2019 levin porterâ wildâ seat histor republican district 2018 kirkpatrick 2018 district parti cook report rate wild seat like democrat seat solid democrat â approach univers virginia center polit claim sent us 2018 race polit scientist alan abramowitz found support medicar vote loser hous race â ocasio cortez campaign also sent list eight member co sponsor r 1384 competit district lost doesnâ directli address claim fact check â â ocasio cortez said everi hous democrat swing seat support medicar race â ocasio cortez statement true far goe leav mislead impress two 23 democrat incumb run electionâ whose race cook polit report rate toss lean democrat sign co sponsor bill rate statement mostli true â jill terreri ramo 11 17 2020[0.18699187 0.01875    0.0950996  0.07958922 0.07734807 0.19891746\\n 0.04392157 0.04959016 0.1507177  0.84820557 0.6027689  0.70173267\\n 0.997      0.9287101  0.04419067 0.05285659 0.04959279 0.25773525]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Complete_Data'][150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2fc9787",
   "metadata": {
    "id": "UtxsAJb1m-w8"
   },
   "outputs": [],
   "source": [
    "for row in range(0,len(df)):\n",
    "    df['Complete_Data'][row] = re.sub(r'\\[',' ',df['Complete_Data'][row])\n",
    "    df['Complete_Data'][row] = re.sub(r'\\]',' ',df['Complete_Data'][row])\n",
    "    df['Complete_Data'][row] = re.sub(r'\\n',' ',df['Complete_Data'][row])\n",
    "    df['Complete_Data'][row] = re.sub(r'\\s+', ' ', df['Complete_Data'][row], flags=re.I)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e362fc",
   "metadata": {
    "id": "HGyMxm6mVkCl"
   },
   "source": [
    "## **Set Max features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6540e17b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEzfuJCxnJBD",
    "outputId": "82a43333-5b64-4632-ed26-125fcdab4d36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 3\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.2-py3-none-any.whl (101 kB)\n",
      "Collecting pyphen\n",
      "  Downloading pyphen-0.11.0-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.11.0 textstat-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d177e7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5cfb51a",
    "outputId": "bafcc972-0e5f-4c91-a834-4f1b70ef24f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:  37\n",
      "max:  3090\n",
      "mean: 459.13\n",
      "mean + 2 *sigma: 965.39\n",
      "965\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "lenght=list()\n",
    "for row in range(0,len(df)):\n",
    "    lenght.insert(row , textstat.lexicon_count(df['Complete_Data'][row]))\n",
    "df['lenght']=lenght\n",
    "\n",
    "print('min: ', min(df['lenght']))\n",
    "print('max: ', max(df['lenght']))\n",
    "\n",
    "print('mean: {:.2f}'.format(np.mean(df['lenght'])))\n",
    "\n",
    "print('mean + 2 *sigma: {:.2f}'.format(np.mean(df['lenght'])+ 2.0 * np.std(df['lenght'])))\n",
    "\n",
    "\n",
    "max_features = int(np.mean(df['lenght'])+ 2.0 * np.std(df['lenght']))\n",
    "print(max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b76cb59",
   "metadata": {
    "id": "643d9b64"
   },
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "df['label'] = encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eca9f0fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvGtb5dczm6H",
    "outputId": "60565963-b3bc-4224-9d23-22bffd6e9951"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FALSE', 'TRUE', 'half-true', 'mostly-false', 'mostly-true'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab0756a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0dc9389",
    "outputId": "cefc42f4-3265-4050-cb7e-174f6df82a47"
   },
   "outputs": [],
   "source": [
    "y = df['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c90f5ac1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLQunBt9PHnx",
    "outputId": "f8147b9d-4aa7-4a88-d790-5f892e335dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21011 1801 7204\n",
      "21011 1801 7204\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.70\n",
    "validation_ratio = 0.10\n",
    "test_ratio = 0.20\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "# the _junk suffix means that we drop that variable completely\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Complete_Data'], y, test_size=0.3)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, train_size=0.2)\n",
    "\n",
    "import tensorflow as tf\n",
    "train_y = tf.keras.utils.to_categorical(y_train, num_classes=5)\n",
    "valid_y = tf.keras.utils.to_categorical(y_val, num_classes=5)\n",
    "test_y = tf.keras.utils.to_categorical(y_test, num_classes=5)\n",
    "\n",
    "print(len(x_train), len(x_val), len(x_test))\n",
    "print(len(train_y), len(valid_y), len(test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d4a316",
   "metadata": {
    "id": "bzreAg3HIZj2"
   },
   "source": [
    "## **word2vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9700f6cb",
   "metadata": {
    "id": "ZXKaGCPvAoVi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Embeddings():\n",
    "    \"\"\"\n",
    "    A class to read the word embedding file and to create the word embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, vector_dimension):\n",
    "        self.path = path \n",
    "        self.vector_dimension = vector_dimension\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_coefs(word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    def get_embedding_index(self):\n",
    "        embeddings_index = dict(self.get_coefs(*o.split(\" \")) for o in open(self.path, errors='ignore'))\n",
    "        return embeddings_index\n",
    "\n",
    "    def create_embedding_matrix(self, tokenizer, vocab_size):\n",
    "        \"\"\"\n",
    "        A method to create the embedding matrix\n",
    "        \"\"\"\n",
    "        model_embed = self.get_embedding_index()\n",
    "\n",
    "        embedding_matrix = np.zeros((vocab_size + 1, self.vector_dimension))\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index > max_features:\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    embedding_matrix[index] = model_embed[word]\n",
    "                except:\n",
    "                    continue\n",
    "        return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee36d5fb",
   "metadata": {
    "id": "mLIeNymoDXjU"
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Complete_Data'])\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "662d24c4",
   "metadata": {
    "id": "7uSlgsw5DGXs"
   },
   "outputs": [],
   "source": [
    "embedding = Embeddings(\n",
    "  'G:/datasets/glove.840B.300d/glove.840B.300d.txt', \n",
    "  vector_dimension = 300\n",
    ")\n",
    "embedding_matrix = embedding.create_embedding_matrix(tokenizer, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de3e61b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Wa1tcLp4uLN",
    "outputId": "d818864e-4686-4e8f-aea6-a96f0dc52569"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005676141204739517"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58750e99",
   "metadata": {
    "id": "81b4135f"
   },
   "outputs": [],
   "source": [
    "padding_type='post'\n",
    "trunc_type='post'\n",
    "\n",
    "#train_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(train_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "#valid_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(valid_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "train_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "test_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "valid_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(x_val), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cc6d487",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_pH6AAZVVYk",
    "outputId": "c371cff1-1f78-4794-c91b-c54f882d38bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21011 7204 1801\n"
     ]
    }
   ],
   "source": [
    "print(len(train_seq_x), len(test_seq_x), len(valid_seq_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d133c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5ec9662",
    "outputId": "855b8346-b5e3-426e-8913-2999c45d50f1"
   },
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('/content/drive/MyDrive/Datasets/Glove.6B/GoogleNews-vectors-negative300.bin',encoding=\"ISO-8859-1\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float64')\n",
    "    \n",
    "#num_word = 10000\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer(oov_token='UNK', filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True)\n",
    "token.fit_on_texts(df['Complete_Data'])\n",
    "word_index = token.word_index\n",
    "print(len(word_index))\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24037b27",
   "metadata": {
    "id": "_r-mLVX3-3xY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wintest\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass classes=[0 1 2 3 4], y=[0 3 0 ... 1 1 1] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "def compute_classweights(target):\n",
    "    \"\"\"\n",
    "    Computes the weights of the target values based on the samples\n",
    "    :param target: Y-target variable\n",
    "    :return: dictionary object\n",
    "    \"\"\"\n",
    "    # compute class weights\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                     np.unique(target),\n",
    "                                                     target)\n",
    "    \n",
    "    # make the class weight list into dictionary\n",
    "    weights = {}\n",
    "    \n",
    "    # enumerate the list\n",
    "    for index, weight in enumerate(class_weights):\n",
    "        weights[index] = weight\n",
    "        \n",
    "    return weights\n",
    "\n",
    "# Get the class weights for the target variable\n",
    "weights = compute_classweights(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84ca5a2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RThPs-YZE_W6",
    "outputId": "becfccd2-a57b-40c1-ce38-57907ed7395e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6269005847953216,\n",
       " 1: 0.628015482791087,\n",
       " 2: 1.4131826741996234,\n",
       " 3: 1.7466395112016293,\n",
       " 4: 1.8783479349186483}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4697a211",
   "metadata": {
    "id": "bd7aec1e"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e3586eb",
   "metadata": {
    "id": "ODi0zfyvxI7o"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "def scheduler(epoch, lr):\n",
    "    \n",
    "    if epoch < 7:\n",
    "        print('-- lr =%f --',lr) \n",
    "        return lr\n",
    "\n",
    "    elif (epoch%7 == 0):\n",
    "        lr = lr * tensorflow.math.exp(-0.2)\n",
    "        print('** lr =%f **',lr)  \n",
    "        return lr\n",
    "        \n",
    "    else:\n",
    "        print('__ lr =%f __',lr) \n",
    "        return lr\n",
    "    \n",
    "    \n",
    "callback = tensorflow.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c7e4c92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8gAPIw2MFI_F",
    "outputId": "3269133c-0229-4d2f-ee13-be211447e9e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 965, 300)          48941700  \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 965, 300)          1200      \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 965, 256)          439296    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 965, 128)          32896     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 49,425,593\n",
      "Trainable params: 483,293\n",
      "Non-trainable params: 48,942,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "embedding_dim =300\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=max_features, weights=[embedding_matrix], trainable=False))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Bidirectional(layers.LSTM(128,return_sequences=True)))\n",
    "model.add(layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(l2=0.002)))\n",
    "model.add(layers.GlobalMaxPool1D()) # Remove flatten layer\n",
    "model.add(layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(l2=0.002)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(l2=0.002)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b2e5b62",
   "metadata": {
    "id": "LIbtZlYUF5Tc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(8e-6), \n",
    "                  loss=losses.CategoricalCrossentropy(), \n",
    "                  metrics=[tf.metrics.AUC(name='accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ca04605",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebqqFgdRGMmi",
    "outputId": "b5491f8f-209c-4039-bec1-bbf1f333ad86",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 21011 samples, validate on 1801 samples\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 1/50\n",
      "21011/21011 [==============================] - 619s 29ms/sample - loss: 2.1514 - accuracy: 0.5989 - val_loss: 1.9254 - val_accuracy: 0.7854\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 2/50\n",
      "21011/21011 [==============================] - 618s 29ms/sample - loss: 1.8945 - accuracy: 0.7546 - val_loss: 1.6967 - val_accuracy: 0.8606\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 3/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 1.6914 - accuracy: 0.8229 - val_loss: 1.5082 - val_accuracy: 0.8885\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 4/50\n",
      "21011/21011 [==============================] - 617s 29ms/sample - loss: 1.5576 - accuracy: 0.8533 - val_loss: 1.3465 - val_accuracy: 0.9034\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 5/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 1.4120 - accuracy: 0.8811 - val_loss: 1.2164 - val_accuracy: 0.9229\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 6/50\n",
      "21011/21011 [==============================] - 618s 29ms/sample - loss: 1.3101 - accuracy: 0.8995 - val_loss: 1.1411 - val_accuracy: 0.9355\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 7/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 1.2457 - accuracy: 0.9134 - val_loss: 1.0821 - val_accuracy: 0.9469\n",
      "** lr =%f ** tf.Tensor(6.549846e-06, shape=(), dtype=float32)\n",
      "Epoch 8/50\n",
      "21011/21011 [==============================] - 617s 29ms/sample - loss: 1.1770 - accuracy: 0.9279 - val_loss: 1.0278 - val_accuracy: 0.9565\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 9/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 1.1223 - accuracy: 0.9378 - val_loss: 0.9796 - val_accuracy: 0.9625\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 10/50\n",
      "21011/21011 [==============================] - 617s 29ms/sample - loss: 1.0682 - accuracy: 0.9481 - val_loss: 0.9304 - val_accuracy: 0.9692\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 11/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 1.0145 - accuracy: 0.9560 - val_loss: 0.8922 - val_accuracy: 0.9726\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 12/50\n",
      "21011/21011 [==============================] - 3833s 182ms/sample - loss: 0.9795 - accuracy: 0.9608 - val_loss: 0.8662 - val_accuracy: 0.9747\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 13/50\n",
      "21011/21011 [==============================] - 615s 29ms/sample - loss: 0.9390 - accuracy: 0.9659 - val_loss: 0.8425 - val_accuracy: 0.9772\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 14/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 0.9133 - accuracy: 0.9687 - val_loss: 0.8246 - val_accuracy: 0.9779\n",
      "** lr =%f ** tf.Tensor(5.3625604e-06, shape=(), dtype=float32)\n",
      "Epoch 15/50\n",
      "21011/21011 [==============================] - 614s 29ms/sample - loss: 0.8850 - accuracy: 0.9712 - val_loss: 0.8163 - val_accuracy: 0.9780\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 16/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 0.8665 - accuracy: 0.9727 - val_loss: 0.8028 - val_accuracy: 0.9792\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 17/50\n",
      "21011/21011 [==============================] - 614s 29ms/sample - loss: 0.8518 - accuracy: 0.9740 - val_loss: 0.7946 - val_accuracy: 0.9795\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 18/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 0.8374 - accuracy: 0.9752 - val_loss: 0.7919 - val_accuracy: 0.9789\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 19/50\n",
      "21011/21011 [==============================] - 614s 29ms/sample - loss: 0.8209 - accuracy: 0.9768 - val_loss: 0.7832 - val_accuracy: 0.9793\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 20/50\n",
      "21011/21011 [==============================] - 617s 29ms/sample - loss: 0.8090 - accuracy: 0.9773 - val_loss: 0.7729 - val_accuracy: 0.9798\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 21/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 0.7938 - accuracy: 0.9786 - val_loss: 0.7667 - val_accuracy: 0.9804\n",
      "** lr =%f ** tf.Tensor(4.3904934e-06, shape=(), dtype=float32)\n",
      "Epoch 22/50\n",
      "21011/21011 [==============================] - 617s 29ms/sample - loss: 0.7833 - accuracy: 0.9793 - val_loss: 0.7630 - val_accuracy: 0.9804\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 23/50\n",
      "21011/21011 [==============================] - 615s 29ms/sample - loss: 0.7766 - accuracy: 0.9796 - val_loss: 0.7575 - val_accuracy: 0.9806\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 24/50\n",
      "21011/21011 [==============================] - 617s 29ms/sample - loss: 0.7664 - accuracy: 0.9804 - val_loss: 0.7519 - val_accuracy: 0.9814\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 25/50\n",
      "21011/21011 [==============================] - 615s 29ms/sample - loss: 0.7583 - accuracy: 0.9810 - val_loss: 0.7493 - val_accuracy: 0.9817\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 26/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 0.7508 - accuracy: 0.9813 - val_loss: 0.7429 - val_accuracy: 0.9814\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 27/50\n",
      "21011/21011 [==============================] - 614s 29ms/sample - loss: 0.7416 - accuracy: 0.9822 - val_loss: 0.7427 - val_accuracy: 0.9816\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 28/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 0.7337 - accuracy: 0.9824 - val_loss: 0.7369 - val_accuracy: 0.9819\n",
      "** lr =%f ** tf.Tensor(3.594632e-06, shape=(), dtype=float32)\n",
      "Epoch 29/50\n",
      "21011/21011 [==============================] - 614s 29ms/sample - loss: 0.7232 - accuracy: 0.9835 - val_loss: 0.7341 - val_accuracy: 0.9817\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 30/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 0.7156 - accuracy: 0.9839 - val_loss: 0.7304 - val_accuracy: 0.9819\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 31/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 0.7093 - accuracy: 0.9841 - val_loss: 0.7296 - val_accuracy: 0.9820\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 32/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 0.7057 - accuracy: 0.9844 - val_loss: 0.7266 - val_accuracy: 0.9817\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 33/50\n",
      "21011/21011 [==============================] - 615s 29ms/sample - loss: 0.7003 - accuracy: 0.9850 - val_loss: 0.7255 - val_accuracy: 0.9823\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 34/50\n",
      "21011/21011 [==============================] - 617s 29ms/sample - loss: 0.6948 - accuracy: 0.9849 - val_loss: 0.7220 - val_accuracy: 0.9818\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 35/50\n",
      "21011/21011 [==============================] - 615s 29ms/sample - loss: 0.6851 - accuracy: 0.9856 - val_loss: 0.7215 - val_accuracy: 0.9817\n",
      "** lr =%f ** tf.Tensor(2.943036e-06, shape=(), dtype=float32)\n",
      "Epoch 36/50\n",
      "21011/21011 [==============================] - 617s 29ms/sample - loss: 0.6815 - accuracy: 0.9856 - val_loss: 0.7169 - val_accuracy: 0.9825\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 37/50\n",
      "21011/21011 [==============================] - 615s 29ms/sample - loss: 0.6777 - accuracy: 0.9858 - val_loss: 0.7178 - val_accuracy: 0.9821\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 38/50\n",
      "21011/21011 [==============================] - 617s 29ms/sample - loss: 0.6755 - accuracy: 0.9859 - val_loss: 0.7151 - val_accuracy: 0.9820\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 39/50\n",
      "21011/21011 [==============================] - 615s 29ms/sample - loss: 0.6696 - accuracy: 0.9862 - val_loss: 0.7140 - val_accuracy: 0.9825\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 40/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 0.6648 - accuracy: 0.9865 - val_loss: 0.7117 - val_accuracy: 0.9824\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 41/50\n",
      "21011/21011 [==============================] - 615s 29ms/sample - loss: 0.6561 - accuracy: 0.9872 - val_loss: 0.7116 - val_accuracy: 0.9820\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 42/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 0.6513 - accuracy: 0.9874 - val_loss: 0.7099 - val_accuracy: 0.9821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** lr =%f ** tf.Tensor(2.409554e-06, shape=(), dtype=float32)\n",
      "Epoch 43/50\n",
      "21011/21011 [==============================] - 615s 29ms/sample - loss: 0.6487 - accuracy: 0.9875 - val_loss: 0.7087 - val_accuracy: 0.9824\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 44/50\n",
      "21011/21011 [==============================] - 617s 29ms/sample - loss: 0.6497 - accuracy: 0.9873 - val_loss: 0.7078 - val_accuracy: 0.9823\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 45/50\n",
      "21011/21011 [==============================] - 615s 29ms/sample - loss: 0.6438 - accuracy: 0.9877 - val_loss: 0.7074 - val_accuracy: 0.9824\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 46/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 0.6376 - accuracy: 0.9882 - val_loss: 0.7051 - val_accuracy: 0.9820\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 47/50\n",
      "21011/21011 [==============================] - 615s 29ms/sample - loss: 0.6312 - accuracy: 0.9885 - val_loss: 0.7079 - val_accuracy: 0.9816\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 48/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 0.6283 - accuracy: 0.9885 - val_loss: 0.7041 - val_accuracy: 0.9822\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 49/50\n",
      "21011/21011 [==============================] - 615s 29ms/sample - loss: 0.6276 - accuracy: 0.9886 - val_loss: 0.7030 - val_accuracy: 0.9830\n",
      "** lr =%f ** tf.Tensor(1.972776e-06, shape=(), dtype=float32)\n",
      "Epoch 50/50\n",
      "21011/21011 [==============================] - 616s 29ms/sample - loss: 0.6288 - accuracy: 0.9886 - val_loss: 0.7046 - val_accuracy: 0.9824\n",
      "Training Accuracy: 0.9932\n",
      "Testing Accuracy:  0.9824\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_seq_x, \n",
    "                    train_y,\n",
    "                    batch_size=4,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_data=(valid_seq_x, valid_y),\n",
    "                    class_weight=weights,\n",
    "                    callbacks=[callback])\n",
    "loss, accuracy = model.evaluate(train_seq_x, train_y, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(valid_seq_x, valid_y, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d323f405",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdeDGbvAskcq",
    "outputId": "d2b7220f-13da-4ed3-8e13-8016ebaa4530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7204/7204 [==============================] - 86s 12ms/sample - loss: 0.6877 - accuracy: 0.9822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6876923868154433, 0.9821647]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_seq_x,test_y, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e6335d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPwL-5MZtfXw",
    "outputId": "17f0b2d7-b248-4ca4-803f-652b1410e1cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 4, ..., 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_LSTM =np.argmax(model.predict(test_seq_x), axis=-1)\n",
    "pred_LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab58b406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 4, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fd5cab8",
   "metadata": {
    "id": "YoNkrLtODBM_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 4, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_eval = y_test\n",
    "y_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab78d68e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfmLcU35tlQw",
    "outputId": "4306b092-0b5a-4f5b-c223-faed0096b34c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88      2259\n",
      "           1       0.97      0.86      0.91      2341\n",
      "           2       0.73      0.84      0.78      1004\n",
      "           3       0.83      0.78      0.81       825\n",
      "           4       0.85      0.88      0.86       775\n",
      "\n",
      "    accuracy                           0.86      7204\n",
      "   macro avg       0.85      0.85      0.85      7204\n",
      "weighted avg       0.87      0.86      0.86      7204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "matrix = classification_report(y_eval,pred_LSTM)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8117758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2044   39   99   66   11]\n",
      " [ 172 2003  110   18   38]\n",
      " [ 106   12  842   27   17]\n",
      " [  65    2   57  646   55]\n",
      " [  16    9   47   21  682]]\n"
     ]
    }
   ],
   "source": [
    "confusionmatrix = confusion_matrix(y_eval,pred_LSTM)\n",
    "print(confusionmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae133ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DNN-LSTM01C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
