{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d81bbc2",
   "metadata": {
    "id": "c0ad506d"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import textblob, string\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import layers, models, optimizers, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c40f695",
   "metadata": {
    "id": "In_zYnx7iyjm"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv('L:/Thesis/Dateset/FeaturesExtraction_Compeleted.csv', encoding='latin-1')\n",
    "\n",
    "df = df.astype(dtype={'title': 'string','url': 'string',\n",
    "                                'date_published': 'string','author_name': 'string',\n",
    "                                'content': 'string','label': 'string'})\n",
    "\n",
    "cols = ['countOfPosWord', 'countOfNegWord', 'NumberOfChar','NumberOfWords','NumberOfSentences','AvgCharPerWord','AvgWordPerSent','NumberOfUpCase','NumberOfPunctuatuion','Orgtitle_contentSim','TopicModellingSim','afinn_score','vader_score_compound','FREI','FKGL','ARI','GFI','CLI']\n",
    "data = pd.read_csv('L:/Thesis/Dateset/FeaturesExtraction_Compeleted.csv',  usecols = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "491932d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FALSE           9576\n",
       "TRUE            9559\n",
       "half-true       4248\n",
       "mostly-false    3437\n",
       "mostly-true     3196\n",
       "Name: label, dtype: Int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91eeb4d2",
   "metadata": {
    "id": "qPaG74F9VoFX"
   },
   "outputs": [],
   "source": [
    "df['text_features']= df['title'] +' '+ df['content'] +' '+ df['author_name'] +' '+ df['date_published']\n",
    "df['Complete_Data'] = df['text_features'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9305119",
   "metadata": {
    "id": "6CXPMvsK2Iif"
   },
   "source": [
    "## **SpellCheker**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d04e8",
   "metadata": {
    "id": "j4HlFVjdHcP7"
   },
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473c316",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHWMrBrJbsg1",
    "outputId": "5ee1ab7f-43f4-4c5f-b020-1b71b8646bea"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d3642a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUyk98dgFhvi",
    "outputId": "17f1c138-ec57-4b63-f89a-41c482feaa21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for row in range(len(df)):\n",
    "  df['Complete_Data'][row] = re.sub(r\"won\\'t\", \"will not\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"can\\'t\", \"can not\", df['Complete_Data'][row])\n",
    "\n",
    "    # general\n",
    "  df['Complete_Data'][row] = re.sub(r\"n\\'t\", \" not\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'re\", \" are\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'s\", \" is\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'d\", \" would\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'ll\", \" will\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'t\", \" not\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'ve\", \" have\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'m\", \" am\", df['Complete_Data'][row])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8585f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "592f0f46",
    "outputId": "3d66939c-728a-4d34-ab7b-9ea973ae71c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "X = df['Complete_Data']\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "import re\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "    document= [word for word in document if word not in stop_words]\n",
    "    document = [ps.stem(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "    \n",
    "df['Complete_Data'] = documents\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0b979f",
   "metadata": {
    "id": "JhOwiMYbI2-a"
   },
   "source": [
    "**Add Numeric features to text features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fe94114",
   "metadata": {
    "id": "YHKQuov7dH_h"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "data_scaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b1a8475",
   "metadata": {
    "id": "kjR7uLY6dlTp"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-99a796ccd68a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnumerical_data1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0mnumerical_data1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_scaled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'numerical_features'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "numerical_data=list()\n",
    "for row in range(0,len(df)):\n",
    "  numerical_data.insert(row,data_scaled[row])\n",
    "df['numerical_features'] = numerical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40e3f13c",
   "metadata": {
    "id": "4lDuf-IydlZ4"
   },
   "outputs": [],
   "source": [
    "df = df.astype(dtype={'numerical_features': 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a6c12b0",
   "metadata": {
    "id": "UtxsAJb1m-w8"
   },
   "outputs": [],
   "source": [
    "for row in range(0,len(df)):\n",
    "    df['numerical_features'][row] = re.sub(r'\\[',' ',df['numerical_features'][row])\n",
    "    df['numerical_features'][row] = re.sub(r'\\]',' ',df['numerical_features'][row])\n",
    "    df['numerical_features'][row] = re.sub(r'\\n',' ',df['numerical_features'][row])\n",
    "    df['numerical_features'][row] = re.sub(r'\\s+', ' ', df['numerical_features'][row], flags=re.I)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5be626",
   "metadata": {
    "id": "HGyMxm6mVkCl"
   },
   "source": [
    "## **Set Max features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef13217",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEzfuJCxnJBD",
    "outputId": "82a43333-5b64-4632-ed26-125fcdab4d36"
   },
   "outputs": [],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8192495c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5cfb51a",
    "outputId": "bafcc972-0e5f-4c91-a834-4f1b70ef24f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:  19\n",
      "max:  3072\n",
      "mean: 442.90\n",
      "mean + 2 *sigma: 949.20\n",
      "949\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "lenght=list()\n",
    "for row in range(0,len(df)):\n",
    "    lenght.insert(row , textstat.lexicon_count(df['Complete_Data'][row]))\n",
    "df['lenght']=lenght\n",
    "\n",
    "print('min: ', min(df['lenght']))\n",
    "print('max: ', max(df['lenght']))\n",
    "\n",
    "print('mean: {:.2f}'.format(np.mean(df['lenght'])))\n",
    "\n",
    "print('mean + 2 *sigma: {:.2f}'.format(np.mean(df['lenght'])+ 2.0 * np.std(df['lenght'])))\n",
    "\n",
    "\n",
    "max_features = int(np.mean(df['lenght'])+ 2.0 * np.std(df['lenght']))\n",
    "print(max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0617e469",
   "metadata": {
    "id": "643d9b64"
   },
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "df['label'] = encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e192c9ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvGtb5dczm6H",
    "outputId": "60565963-b3bc-4224-9d23-22bffd6e9951"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FALSE', 'TRUE', 'half-true', 'mostly-false', 'mostly-true'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc574ba0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0dc9389",
    "outputId": "cefc42f4-3265-4050-cb7e-174f6df82a47"
   },
   "outputs": [],
   "source": [
    "y = df['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c0fd1",
   "metadata": {
    "id": "bzreAg3HIZj2"
   },
   "source": [
    "## **word2vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "174fe688",
   "metadata": {
    "id": "ZXKaGCPvAoVi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Embeddings():\n",
    "    \"\"\"\n",
    "    A class to read the word embedding file and to create the word embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, vector_dimension):\n",
    "        self.path = path \n",
    "        self.vector_dimension = vector_dimension\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_coefs(word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    def get_embedding_index(self):\n",
    "        embeddings_index = dict(self.get_coefs(*o.split(\" \")) for o in open(self.path, errors='ignore'))\n",
    "        return embeddings_index\n",
    "\n",
    "    def create_embedding_matrix(self, tokenizer, vocab_size):\n",
    "        \"\"\"\n",
    "        A method to create the embedding matrix\n",
    "        \"\"\"\n",
    "        model_embed = self.get_embedding_index()\n",
    "\n",
    "        embedding_matrix = np.zeros((vocab_size + 1, self.vector_dimension))\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index > max_features:\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    embedding_matrix[index] = model_embed[word]\n",
    "                except:\n",
    "                    continue\n",
    "        return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6b99a57",
   "metadata": {
    "id": "mLIeNymoDXjU"
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Complete_Data'])\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40ecba5d",
   "metadata": {
    "id": "7uSlgsw5DGXs"
   },
   "outputs": [],
   "source": [
    "embedding = Embeddings(\n",
    "  'G:/datasets/glove.840B.300d/glove.840B.300d.txt', \n",
    "  vector_dimension = 300\n",
    ")\n",
    "embedding_matrix = embedding.create_embedding_matrix(tokenizer, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c73173f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Wa1tcLp4uLN",
    "outputId": "d818864e-4686-4e8f-aea6-a96f0dc52569"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007865644421010778"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cde237ab",
   "metadata": {
    "id": "81b4135f"
   },
   "outputs": [],
   "source": [
    "padding_type='pre'\n",
    "trunc_type='pre'\n",
    "\n",
    "#train_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(train_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "#valid_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(valid_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "data = sequence.pad_sequences(tokenizer.texts_to_sequences(df['Complete_Data']), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "\n",
    "#train_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "#test_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "#valid_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(x_val), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "122b06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "com = np.hstack((data,data_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "467302bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30016, 967)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "920ad81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21011 1801 7204\n",
      "21011 1801 7204\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.70\n",
    "validation_ratio = 0.10\n",
    "test_ratio = 0.20\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "# the _junk suffix means that we drop that variable completely\n",
    "x_train, x_test, y_train, y_test = train_test_split(com, y, test_size=0.3)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, train_size=0.2)\n",
    "\n",
    "import tensorflow as tf\n",
    "train_y = tf.keras.utils.to_categorical(y_train, num_classes=5)\n",
    "valid_y = tf.keras.utils.to_categorical(y_val, num_classes=5)\n",
    "test_y = tf.keras.utils.to_categorical(y_test, num_classes=5)\n",
    "\n",
    "print(len(x_train), len(x_val), len(x_test))\n",
    "print(len(train_y), len(valid_y), len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0ff85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5ec9662",
    "outputId": "855b8346-b5e3-426e-8913-2999c45d50f1"
   },
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('/content/drive/MyDrive/Datasets/Glove.6B/GoogleNews-vectors-negative300.bin',encoding=\"ISO-8859-1\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float64')\n",
    "    \n",
    "#num_word = 10000\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer(oov_token='UNK', filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True)\n",
    "token.fit_on_texts(df['Complete_Data'])\n",
    "word_index = token.word_index\n",
    "print(len(word_index))\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d14d3824",
   "metadata": {
    "id": "_r-mLVX3-3xY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wintest\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass classes=[0 1 2 3 4], y=[0 3 0 ... 1 1 1] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "def compute_classweights(target):\n",
    "    \"\"\"\n",
    "    Computes the weights of the target values based on the samples\n",
    "    :param target: Y-target variable\n",
    "    :return: dictionary object\n",
    "    \"\"\"\n",
    "    # compute class weights\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                     np.unique(target),\n",
    "                                                     target)\n",
    "    \n",
    "    # make the class weight list into dictionary\n",
    "    weights = {}\n",
    "    \n",
    "    # enumerate the list\n",
    "    for index, weight in enumerate(class_weights):\n",
    "        weights[index] = weight\n",
    "        \n",
    "    return weights\n",
    "\n",
    "# Get the class weights for the target variable\n",
    "weights = compute_classweights(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b9f345a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RThPs-YZE_W6",
    "outputId": "becfccd2-a57b-40c1-ce38-57907ed7395e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6269005847953216,\n",
       " 1: 0.628015482791087,\n",
       " 2: 1.4131826741996234,\n",
       " 3: 1.7466395112016293,\n",
       " 4: 1.8783479349186483}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "28b63474",
   "metadata": {
    "id": "bd7aec1e"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3111c731",
   "metadata": {
    "id": "ODi0zfyvxI7o"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "def scheduler(epoch, lr):\n",
    "    \n",
    "    if epoch < 7:\n",
    "        print('-- lr =%f --',lr) \n",
    "        return lr\n",
    "\n",
    "    elif (epoch%7 == 0):\n",
    "        lr = lr * tensorflow.math.exp(-0.2)\n",
    "        print('** lr =%f **',lr)  \n",
    "        return lr\n",
    "        \n",
    "    else:\n",
    "        print('__ lr =%f __',lr) \n",
    "        return lr\n",
    "    \n",
    "    \n",
    "callback = tensorflow.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5eced527",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'value_counts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-e20da9714e8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'value_counts'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bbf6da0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8IL6VCFwG5UT",
    "outputId": "2520a715-65cc-4b81-94ec-8afb9bdb6d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 967, 300)          34707900  \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 967, 300)          1200      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 965, 256)          230656    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 965, 128)          32896     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 34,983,153\n",
      "Trainable params: 274,653\n",
      "Non-trainable params: 34,708,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import losses\n",
    "\n",
    "\n",
    "embedding_dim =300\n",
    "model_ConV = Sequential()\n",
    "model_ConV.add(layers.Embedding(vocab_size, embedding_dim, input_length=max_features+18, weights=[embedding_matrix], trainable=False))\n",
    "model_ConV.add(layers.BatchNormalization())\n",
    "model_ConV.add(layers.Conv1D(256,3))\n",
    "model_ConV.add(layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(l2=0.002)))\n",
    "model_ConV.add(layers.GlobalMaxPool1D()) # Remove flatten layer\n",
    "model_ConV.add(layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(l2=0.002)))\n",
    "model_ConV.add(layers.Dropout(0.3))\n",
    "model_ConV.add(layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(l2=0.002)))\n",
    "model_ConV.add(layers.Dropout(0.3))\n",
    "model_ConV.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model_ConV.summary()\n",
    "\n",
    "model_ConV.compile(optimizer=tf.keras.optimizers.Adam(1e-4), \n",
    "                  loss=losses.CategoricalCrossentropy(), \n",
    "                  metrics=[tf.metrics.AUC(name='accuracy')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "90092cdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9oH4jDcvHV9-",
    "outputId": "bd568eee-538a-4e0b-d4d3-9cfa7f51e433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 21011 samples, validate on 1801 samples\n",
      "-- lr =%f -- 9.999999747378752e-05\n",
      "Epoch 1/50\n",
      "21011/21011 [==============================] - 119s 6ms/sample - loss: 1.9377 - accuracy: 0.7627 - val_loss: 1.3813 - val_accuracy: 0.8999\n",
      "-- lr =%f -- 9.999999747378752e-05\n",
      "Epoch 2/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 1.2249 - accuracy: 0.9309 - val_loss: 0.9670 - val_accuracy: 0.9632\n",
      "-- lr =%f -- 9.999999747378752e-05\n",
      "Epoch 3/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.9688 - accuracy: 0.9584 - val_loss: 0.8483 - val_accuracy: 0.9725\n",
      "-- lr =%f -- 9.999999747378752e-05\n",
      "Epoch 4/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.8526 - accuracy: 0.9660 - val_loss: 0.8061 - val_accuracy: 0.9705\n",
      "-- lr =%f -- 9.999999747378752e-05\n",
      "Epoch 5/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.7691 - accuracy: 0.9706 - val_loss: 0.7389 - val_accuracy: 0.9742\n",
      "-- lr =%f -- 9.999999747378752e-05\n",
      "Epoch 6/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.7163 - accuracy: 0.9728 - val_loss: 0.7377 - val_accuracy: 0.9669\n",
      "-- lr =%f -- 9.999999747378752e-05\n",
      "Epoch 7/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.6701 - accuracy: 0.9756 - val_loss: 0.6844 - val_accuracy: 0.9752\n",
      "** lr =%f ** tf.Tensor(8.187308e-05, shape=(), dtype=float32)\n",
      "Epoch 8/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.6303 - accuracy: 0.9775 - val_loss: 0.6489 - val_accuracy: 0.9782\n",
      "__ lr =%f __ 8.187307685147971e-05\n",
      "Epoch 9/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.5926 - accuracy: 0.9797 - val_loss: 0.6409 - val_accuracy: 0.9779\n",
      "__ lr =%f __ 8.187307685147971e-05\n",
      "Epoch 10/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.5653 - accuracy: 0.9813 - val_loss: 0.6157 - val_accuracy: 0.9753\n",
      "__ lr =%f __ 8.187307685147971e-05\n",
      "Epoch 11/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.5417 - accuracy: 0.9823 - val_loss: 0.6314 - val_accuracy: 0.9768\n",
      "__ lr =%f __ 8.187307685147971e-05\n",
      "Epoch 12/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.5125 - accuracy: 0.9841 - val_loss: 0.6154 - val_accuracy: 0.9775\n",
      "__ lr =%f __ 8.187307685147971e-05\n",
      "Epoch 13/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.4963 - accuracy: 0.9850 - val_loss: 0.6278 - val_accuracy: 0.9769\n",
      "__ lr =%f __ 8.187307685147971e-05\n",
      "Epoch 14/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.4793 - accuracy: 0.9858 - val_loss: 0.5974 - val_accuracy: 0.9782\n",
      "** lr =%f ** tf.Tensor(6.703201e-05, shape=(), dtype=float32)\n",
      "Epoch 15/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.4485 - accuracy: 0.9880 - val_loss: 0.6211 - val_accuracy: 0.9792\n",
      "__ lr =%f __ 6.70320077915676e-05\n",
      "Epoch 16/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.4292 - accuracy: 0.9889 - val_loss: 0.6187 - val_accuracy: 0.9785\n",
      "__ lr =%f __ 6.70320077915676e-05\n",
      "Epoch 17/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.4102 - accuracy: 0.9904 - val_loss: 0.6255 - val_accuracy: 0.9768\n",
      "__ lr =%f __ 6.70320077915676e-05\n",
      "Epoch 18/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.3974 - accuracy: 0.9906 - val_loss: 0.6180 - val_accuracy: 0.9766\n",
      "__ lr =%f __ 6.70320077915676e-05\n",
      "Epoch 19/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.3830 - accuracy: 0.9915 - val_loss: 0.6094 - val_accuracy: 0.9765\n",
      "__ lr =%f __ 6.70320077915676e-05\n",
      "Epoch 20/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.3710 - accuracy: 0.9921 - val_loss: 0.6964 - val_accuracy: 0.9753\n",
      "__ lr =%f __ 6.70320077915676e-05\n",
      "Epoch 21/50\n",
      "21011/21011 [==============================] - 106s 5ms/sample - loss: 0.3589 - accuracy: 0.9927 - val_loss: 0.6575 - val_accuracy: 0.9726\n",
      "** lr =%f ** tf.Tensor(5.4881166e-05, shape=(), dtype=float32)\n",
      "Epoch 22/50\n",
      "21011/21011 [==============================] - 104s 5ms/sample - loss: 0.3358 - accuracy: 0.9939 - val_loss: 0.6695 - val_accuracy: 0.9746\n",
      "__ lr =%f __ 5.488116585183889e-05\n",
      "Epoch 23/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.3218 - accuracy: 0.9944 - val_loss: 0.6708 - val_accuracy: 0.9744\n",
      "__ lr =%f __ 5.488116585183889e-05\n",
      "Epoch 24/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.3116 - accuracy: 0.9949 - val_loss: 0.7666 - val_accuracy: 0.9698\n",
      "__ lr =%f __ 5.488116585183889e-05\n",
      "Epoch 25/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.3048 - accuracy: 0.9950 - val_loss: 0.7288 - val_accuracy: 0.9714\n",
      "__ lr =%f __ 5.488116585183889e-05\n",
      "Epoch 26/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.2930 - accuracy: 0.9955 - val_loss: 0.7468 - val_accuracy: 0.9719\n",
      "__ lr =%f __ 5.488116585183889e-05\n",
      "Epoch 27/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.2838 - accuracy: 0.9961 - val_loss: 0.6822 - val_accuracy: 0.9722\n",
      "__ lr =%f __ 5.488116585183889e-05\n",
      "Epoch 28/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.2766 - accuracy: 0.9962 - val_loss: 0.7128 - val_accuracy: 0.9703\n",
      "** lr =%f ** tf.Tensor(4.4932898e-05, shape=(), dtype=float32)\n",
      "Epoch 29/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.2550 - accuracy: 0.9971 - val_loss: 0.8349 - val_accuracy: 0.9688\n",
      "__ lr =%f __ 4.493289816309698e-05\n",
      "Epoch 30/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.2507 - accuracy: 0.9972 - val_loss: 0.7847 - val_accuracy: 0.9692\n",
      "__ lr =%f __ 4.493289816309698e-05\n",
      "Epoch 31/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.2415 - accuracy: 0.9975 - val_loss: 0.7855 - val_accuracy: 0.9692\n",
      "__ lr =%f __ 4.493289816309698e-05\n",
      "Epoch 32/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.2301 - accuracy: 0.9978 - val_loss: 0.8048 - val_accuracy: 0.9630\n",
      "__ lr =%f __ 4.493289816309698e-05\n",
      "Epoch 33/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.2273 - accuracy: 0.9979 - val_loss: 0.9117 - val_accuracy: 0.9628\n",
      "__ lr =%f __ 4.493289816309698e-05\n",
      "Epoch 34/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.2271 - accuracy: 0.9979 - val_loss: 0.8663 - val_accuracy: 0.9652\n",
      "__ lr =%f __ 4.493289816309698e-05\n",
      "Epoch 35/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.2223 - accuracy: 0.9980 - val_loss: 0.9122 - val_accuracy: 0.9623\n",
      "** lr =%f ** tf.Tensor(3.6787947e-05, shape=(), dtype=float32)\n",
      "Epoch 36/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.2047 - accuracy: 0.9985 - val_loss: 0.9148 - val_accuracy: 0.9610\n",
      "__ lr =%f __ 3.678794746519998e-05\n",
      "Epoch 37/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.2010 - accuracy: 0.9985 - val_loss: 0.8653 - val_accuracy: 0.9592\n",
      "__ lr =%f __ 3.678794746519998e-05\n",
      "Epoch 38/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.1986 - accuracy: 0.9985 - val_loss: 0.8984 - val_accuracy: 0.9630\n",
      "__ lr =%f __ 3.678794746519998e-05\n",
      "Epoch 39/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.1940 - accuracy: 0.9987 - val_loss: 0.9122 - val_accuracy: 0.9609\n",
      "__ lr =%f __ 3.678794746519998e-05\n",
      "Epoch 40/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.1875 - accuracy: 0.9989 - val_loss: 0.9272 - val_accuracy: 0.9630\n",
      "__ lr =%f __ 3.678794746519998e-05\n",
      "Epoch 41/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.1858 - accuracy: 0.9990 - val_loss: 0.9240 - val_accuracy: 0.9588\n",
      "__ lr =%f __ 3.678794746519998e-05\n",
      "Epoch 42/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.1827 - accuracy: 0.9990 - val_loss: 0.9772 - val_accuracy: 0.9597\n",
      "** lr =%f ** tf.Tensor(3.0119425e-05, shape=(), dtype=float32)\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.1741 - accuracy: 0.9991 - val_loss: 1.0241 - val_accuracy: 0.9562\n",
      "__ lr =%f __ 3.0119424991426058e-05\n",
      "Epoch 44/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.1688 - accuracy: 0.9993 - val_loss: 0.9890 - val_accuracy: 0.9584\n",
      "__ lr =%f __ 3.0119424991426058e-05\n",
      "Epoch 45/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.1634 - accuracy: 0.9993 - val_loss: 0.9683 - val_accuracy: 0.9608\n",
      "__ lr =%f __ 3.0119424991426058e-05\n",
      "Epoch 46/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.1651 - accuracy: 0.9993 - val_loss: 1.0438 - val_accuracy: 0.9534\n",
      "__ lr =%f __ 3.0119424991426058e-05\n",
      "Epoch 47/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.1641 - accuracy: 0.9993 - val_loss: 0.9913 - val_accuracy: 0.9597\n",
      "__ lr =%f __ 3.0119424991426058e-05\n",
      "Epoch 48/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.1546 - accuracy: 0.9995 - val_loss: 1.0714 - val_accuracy: 0.9557\n",
      "__ lr =%f __ 3.0119424991426058e-05\n",
      "Epoch 49/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.1568 - accuracy: 0.9993 - val_loss: 1.0914 - val_accuracy: 0.9550\n",
      "** lr =%f ** tf.Tensor(2.46597e-05, shape=(), dtype=float32)\n",
      "Epoch 50/50\n",
      "21011/21011 [==============================] - 103s 5ms/sample - loss: 0.1525 - accuracy: 0.9995 - val_loss: 1.0933 - val_accuracy: 0.9551\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_seq_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-c815b2f904e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                     callbacks=[callback])\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_ConV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_seq_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Accuracy: {:.4f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_ConV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_seq_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_seq_x' is not defined"
     ]
    }
   ],
   "source": [
    "history = model_ConV.fit(x_train, \n",
    "                    train_y,\n",
    "                    batch_size=4,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, valid_y),\n",
    "                    class_weight=weights,\n",
    "                    callbacks=[callback])\n",
    "loss, accuracy = model_ConV.evaluate(train_seq_x, train_y, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_ConV.evaluate(valid_seq_x, valid_y, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "86a4a5b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wp0atXVuB8ES",
    "outputId": "4e268170-a6b2-4e40-bbd0-894bf90f6c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7204/7204 [==============================] - 14s 2ms/sample - loss: 0.8987 - accuracy: 0.9558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.898650554783936, 0.9558065]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model_ConV.evaluate(x_test,test_y,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8a095800",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "id": "7jPLiui6EAto",
    "outputId": "ba345f6e-8743-4b06-eb4d-7948cc5d27e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 2, 3, 0], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pred_conv =np.argmax(model_ConV.predict(x_test), axis=-1)\n",
    "pred_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1f3843cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HrGMVUs1EJvI",
    "outputId": "de09acf2-5c15-4644-b294-0c6ff52f9085"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 4, ..., 3, 3, 0], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_eval_conv = np.argmax(test_y, axis=-1)\n",
    "y_eval_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "64da97ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAg8CbZrEM_4",
    "outputId": "39765267-6316-449b-ca75-50d77bbab100"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87      2236\n",
      "           1       0.92      0.89      0.90      2343\n",
      "           2       0.78      0.79      0.78      1013\n",
      "           3       0.85      0.74      0.79       814\n",
      "           4       0.83      0.86      0.84       798\n",
      "\n",
      "    accuracy                           0.86      7204\n",
      "   macro avg       0.84      0.83      0.84      7204\n",
      "weighted avg       0.86      0.86      0.86      7204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "matrix = classification_report(y_eval_conv,pred_conv)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4f1fd40c",
   "metadata": {
    "id": "GmPwll219g6o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2003   86   78   51   18]\n",
      " [ 146 2087   61   10   39]\n",
      " [ 132   54  796   11   20]\n",
      " [  85   20   43  599   67]\n",
      " [  23   24   38   30  683]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_conv = confusion_matrix(y_eval_conv,pred_conv)\n",
    "print(confusion_matrix_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb42ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DNN-LSTM01C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
