{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dacDa7aGjxsZ"
   },
   "source": [
    "# **Preparing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load Dataset\n",
    "dataset=pd.read_csv(\"C:/Users/Wintest/Desktop/FakenewsNet/Complete.csv\")\n",
    "\n",
    "dataset = dataset.astype(dtype={'title': 'string',\n",
    "                                'author_name': 'string',\n",
    "                                'content': 'string','label': 'string'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicateRowsDF = dataset[dataset.duplicated()]\n",
    "print(\"Duplicate Rows except first occurrence based on all columns are :\")\n",
    "print(duplicateRowsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WVJITGPbwYd"
   },
   "source": [
    "# **Semantic Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCl4IC4EWUAb"
   },
   "source": [
    "## **Quantity Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fQ8ZNy6IAdF"
   },
   "outputs": [],
   "source": [
    "#Calculate Number of Postive and Negative Words\n",
    "\n",
    "PosWords = open('L:/Thesis/Dateset/PostiveWord.txt' , 'r')\n",
    "NegWords = open('L:/Thesis/Dateset/NegativeWords.txt' , 'r')\n",
    "\n",
    "PosWordSet = list (PosWords)\n",
    "NegWordSet = list (NegWords)\n",
    "\n",
    "countOfPosWord = list()\n",
    "countOfNegWord = list()\n",
    "\n",
    "\n",
    "for row in range(0,len(dataset)):\n",
    "  NumOfPosWord=0\n",
    "  word = dataset['content'][row].split()\n",
    "  for i in range(0,len(PosWordSet)):\n",
    "    PosWordSet[i] = PosWordSet[i].replace('\\n','')\n",
    "    for j in range(0,len(word)):\n",
    "      if word[j] == PosWordSet[i]:\n",
    "        NumOfPosWord +=1\n",
    "  countOfPosWord.insert(row,NumOfPosWord)\n",
    "  NumOfNegWord=0\n",
    "  word = dataset['content'][row].split()\n",
    "  for i in range(0,len(NegWordSet)):\n",
    "    NegWordSet[i] = NegWordSet[i].replace('\\n','')\n",
    "    for j in range(0,len(word)):\n",
    "      if word[j] == NegWordSet[i]:\n",
    "        NumOfNegWord +=1\n",
    "  countOfNegWord.insert(row,NumOfNegWord)\n",
    "\n",
    "dataset['countOfPosWord'] = countOfPosWord\n",
    "dataset['countOfNegWord'] = countOfNegWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4MF36Xx7eurC",
    "outputId": "3a8fa231-19ad-4841-bc8a-5c7983b52e0f"
   },
   "outputs": [],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XYxnd3jqGvP"
   },
   "outputs": [],
   "source": [
    "NumOfChar = list()\n",
    "NumOfWord = list()\n",
    "NumOfSen = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttFqIgFwb7s5",
    "outputId": "10eb1d8c-7ebf-45be-ffb2-9d94fd12a12d"
   },
   "outputs": [],
   "source": [
    "import textstat\n",
    "#Calculate number of characters\n",
    "\n",
    "for row in range(0,len(dataset)):\n",
    "  NumOfChar.insert(row,textstat.char_count(dataset['content'][row]))\n",
    "\n",
    "#Calculate number of words\n",
    "for row in range(0,len(dataset)):\n",
    "  NumOfWord.insert(row,textstat.lexicon_count(dataset['content'][row]))\n",
    "\n",
    "#Calculate number of sentences\n",
    "\n",
    "for row in range(0,len(dataset)):\n",
    "  NumOfSen.insert(row,textstat.sentence_count(dataset['content'][row]))\n",
    "\n",
    "dataset['NumberOfChar']=NumOfChar\n",
    "dataset['NumberOfWords']=NumOfWord\n",
    "dataset['NumberOfSentences']=NumOfSen\n",
    "\n",
    "AvgCharPerWord=list()\n",
    "AvgWordPerSent=list()\n",
    "\n",
    "for row in range(0,len(dataset)):\n",
    "  AvgCharPerWord.insert(row,textstat.avg_character_per_word(dataset['content'][row]))\n",
    "\n",
    "for i in range(0,len(NumOfChar)):\n",
    "    AvgWordPerSent.insert(i,(NumOfWord[i] / NumOfSen[i]))\n",
    "\n",
    "\n",
    "dataset['AvgCharPerWord']=AvgCharPerWord\n",
    "dataset['AvgWordPerSent']=AvgWordPerSent\n",
    "\n",
    "NumberOfUpCase=list()\n",
    "for row in range(0,len(dataset)):\n",
    "  uppercase_count=0  \n",
    "  for character in dataset['content'][row]:\n",
    "      if character.isupper():\n",
    "          uppercase_count += 1\n",
    "  NumberOfUpCase.insert(row,uppercase_count)\n",
    "print(row)\n",
    "dataset['NumberOfUpCase']=NumberOfUpCase\n",
    "\n",
    "#Calculate Number of punctuations\n",
    "import string\n",
    "puncsigns=string.punctuation\n",
    "NumOfPunc=list()\n",
    "for row in range(0,len(dataset)):\n",
    "  Punc=0\n",
    "  for char in dataset['content'][row]:\n",
    "    if char in puncsigns:\n",
    "      Punc = Punc+1\n",
    "  NumOfPunc.insert(row,Punc)\n",
    "dataset['NumberOfPunctuatuion']=NumOfPunc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DIo73nSimEIg",
    "outputId": "b2c88661-5ae4-463c-ae00-c18911eeff8e"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQNZpfXWw1Wa"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "dataset.to_csv('Export_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xy05FOh0GRdF"
   },
   "source": [
    "## **Topic Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SSGthvj9yS4",
    "outputId": "0847b23f-9ae1-407c-9eb6-410238f488b3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"/content/drive/MyDrive/Thesis Datasets/Export_dataset.csv\")\n",
    "dataset.set_index('id')\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCTazSvgC1Zo"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_T0ZT5pC9fv",
    "outputId": "16d7efc1-6671-40d0-f23d-9b1035ba092f"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "atNRZcASGmAN",
    "outputId": "9a36c402-2238-4e45-80ef-59343ded5797"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzCi0TjqDDk4"
   },
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSm8A0E-HhqD"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import gensim\n",
    "\n",
    "def topicModelling(content):\n",
    "  text_data = []\n",
    "  while(True):\n",
    "      tokens = prepare_text_for_lda(content)\n",
    "      if random.random() > .99:\n",
    "          #print(tokens)\n",
    "          text_data.append(tokens)\n",
    "          break;\n",
    "  dictionary = corpora.Dictionary(text_data)\n",
    "  corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "  pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "  dictionary.save('c:/users/wintest/desktop/dictionary.gensim')\n",
    "  NUM_TOPICS = 10\n",
    "  if len(corpus[0]) == 0:\n",
    "    return ('none')\n",
    "  else:\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "    my_dict = {'Topic_' + str(i): [token for token, score in ldamodel.show_topic(i, topn=10)] for i in range(0, ldamodel.num_topics)}\n",
    "    print(my_dict['Topic_9'])\n",
    "  \n",
    "    return(my_dict['Topic_9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hLD2FEa4IRA5",
    "outputId": "58bea89c-1a1e-4b83-9359-425463740306"
   },
   "outputs": [],
   "source": [
    "contentTopicModelling = list()\n",
    "\n",
    "for row in range(0,len(dataset)):\n",
    "  print(row)\n",
    "\n",
    "  topicmodelling = topicModelling(dataset['content'][row])\n",
    "\n",
    "  if topicmodelling  == 'none':\n",
    "    contentTopicModelling.insert(row,'Null')\n",
    "    #dataset = dataset.drop(dataset.index[row])\n",
    "    print('Deleted empty row')\n",
    "    \n",
    "  else:\n",
    "    contentTopicModelling.insert(row,topicmodelling)\n",
    "\n",
    "dataset['ContentTM'] = contentTopicModelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbAqZNq-Cbm8"
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset.to_csv('Export_dataset2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mKtYgxNTJZQ"
   },
   "source": [
    "## **Semantic Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNqapGtJYkRW",
    "outputId": "5996bb99-0005-462f-b6a8-99af76ab7558"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"D:/Rezaei/Export_dataset2.csv\")\n",
    "dataset.set_index('id')\n",
    "dataset = dataset.astype(dtype={'title': 'string','url': 'string',\n",
    "                                'date_published': 'string','author_name': 'string',\n",
    "                                'content': 'string','label': 'string','ContentTM':'string'})\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXK3QayXyl6F",
    "outputId": "dab27bb7-1041-43d5-f26f-9c2a37b74864"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "global documents_df\n",
    "#tfidfvectoriser=int()\n",
    "#vocab_size=int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iu9ySu5GyqL9"
   },
   "outputs": [],
   "source": [
    "def preprocssing(documents_df):\n",
    "  stop_words_l=stopwords.words('english')\n",
    "  documents_df['documents_cleaned']=documents_df.documents.apply(lambda x: \" \".join(re.sub(r'[^a-zA-Z]',' ',w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stop_words_l) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ex3f28AC3tjO"
   },
   "outputs": [],
   "source": [
    "def TfidfSim(documents_df):\n",
    "    global tfidfvectoriser\n",
    "    tfidfvectoriser=TfidfVectorizer(max_features=64)\n",
    "    tfidfvectoriser.fit(documents_df.documents_cleaned)\n",
    "    global tfidf_vectors\n",
    "    tfidf_vectors=tfidfvectoriser.transform(documents_df.documents_cleaned)\n",
    "    tfidf_vectors=tfidf_vectors.toarray()\n",
    "    pairwise_similarities=np.dot(tfidf_vectors,tfidf_vectors.T)\n",
    "    return(pairwise_similarities[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fIQjZu76XWRQ",
    "outputId": "8440a824-a646-4155-bc4e-baca0b7e65d0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import gensim\n",
    "from gensim import models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def word2vecSim(documents_df):\n",
    "  global tokenizer\n",
    "  tokenizer=Tokenizer()\n",
    "  tokenizer.fit_on_texts(documents_df.documents_cleaned)\n",
    "  tokenized_documents=tokenizer.texts_to_sequences(documents_df.documents_cleaned)\n",
    "  global tokenized_paded_documents\n",
    "  tokenized_paded_documents=pad_sequences(tokenized_documents,maxlen=64,padding='post')\n",
    "  global vocab_size\n",
    "  vocab_size=len(tokenizer.word_index)+1\n",
    "  # loading pre-trained embeddings, each word is represented as a 300 dimensional vector\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVYHwhuq1hWi"
   },
   "outputs": [],
   "source": [
    "def gloveSim(documents_df):\n",
    "  # reading Glove word embeddings into a dictionary with \"word\" as key and values as word vectors\n",
    "\n",
    "  embeddings_index = dict()\n",
    "\n",
    "  with open('G:/datasets/glove.6B/glove.6B.50d.txt', encoding='utf-8') as file:\n",
    "      for line in file:\n",
    "          values = line.split()\n",
    "          word = values[0]\n",
    "          coefs = np.asarray(values[1:], dtype='float64')\n",
    "          embeddings_index[word] = coefs\n",
    "  # creating embedding matrix, every row is a vector representation from the vocabulary indexed by the tokenizer index. \n",
    "\n",
    "  embedding_matrix=np.zeros((vocab_size,50))\n",
    "\n",
    "  for word,i in tokenizer.word_index.items():\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "          embedding_matrix[i] = embedding_vector\n",
    "  # tf-idf vectors do not keep the original sequence of words, converting them into actual word sequences from the documents\n",
    "\n",
    "  document_embeddings=np.zeros((len(tokenized_paded_documents),50))\n",
    "  words=tfidfvectoriser.get_feature_names()\n",
    "\n",
    "  for i in range(documents_df.shape[0]):\n",
    "      for j in range(len(words)):\n",
    "          document_embeddings[i]+=embedding_matrix[tokenizer.word_index[words[j]]]*tfidf_vectors[i][j]\n",
    "        \n",
    "  document_embeddings=document_embeddings/np.sum(tfidf_vectors,axis=1).reshape(-1,1)\n",
    "  pairwise_similarities=cosine_similarity(document_embeddings)\n",
    "  return(pairwise_similarities[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8WWG6Za2Tgb"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def bertSim(documents_df):\n",
    "  sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "  document_embeddings = sbert_model.encode(documents_df['documents_cleaned'])\n",
    "  pairwise_similarities=cosine_similarity(document_embeddings)\n",
    "  return(pairwise_similarities[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lki5yB53Uz4y"
   },
   "source": [
    "### **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1-fh90G-2jWd",
    "outputId": "94bb3ca3-3d3f-458f-9e4f-65254b5f7ad3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data=pd.read_csv('/content/drive/MyDrive/Thesis Datasets/T1.csv')\n",
    "similarity=list()\n",
    "for row in range(0,len(dataset)):\n",
    "  try:\n",
    "    documents = [dataset['title'][row],dataset['content'][row],]\n",
    "    pd.set_option('display.max_colwidth', 0)\n",
    "    pd.set_option('display.max_columns', 0)\n",
    "    documents_df=pd.DataFrame(documents,columns=['documents'])\n",
    "    preprocssing(documents_df)\n",
    "    tfidfsim=TfidfSim(documents_df)\n",
    "    word2vecsim=word2vecSim(documents_df)\n",
    "    glovesim=gloveSim(documents_df)\n",
    "    bertsim=bertSim(documents_df)\n",
    "    avgSimilarity=(tfidfsim+glovesim+bertsim)/3\n",
    "    print(avgSimilarity,\"-->\",row)\n",
    "    similarity.insert(row,avgSimilarity)\n",
    "  except ValueError:\n",
    "    similarity.insert(row,\"ValueError\")\n",
    "dataset['Orgtitle_contentSim']=similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('Export_dataset3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWFl0K4am-GB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "similarityTM=list()\n",
    "for row in range(0,len(dataset)):\n",
    "  try:\n",
    "    documents = [dataset['title'][row],dataset['ContentTM'][row],]\n",
    "    pd.set_option('display.max_colwidth', 0)\n",
    "    pd.set_option('display.max_columns', 0)\n",
    "    documents_df=pd.DataFrame(documents,columns=['documents'])\n",
    "    preprocssing(documents_df)\n",
    "    tfidfsim=TfidfSim(documents_df)\n",
    "    word2vecsim=word2vecSim(documents_df)\n",
    "    glovesim=gloveSim(documents_df)\n",
    "    bertsim=bertSim(documents_df)\n",
    "    avgSimilarity=(tfidfsim+glovesim+bertsim)/3\n",
    "    print(avgSimilarity,\"-->\",row)\n",
    "    similarityTM.insert(row,avgSimilarity)\n",
    "  except ValueError:\n",
    "    similarityTM.insert(row,\"ValueError\")\n",
    "dataset['TopicModellingSim'] = similarityTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOscXMA9rnFT"
   },
   "outputs": [],
   "source": [
    "dataset.to_csv('Export_dataset10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyMn1LMAMdEK"
   },
   "source": [
    "# **Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocpDed3iMfVo"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoI07JYjMjrv"
   },
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "afinn = Afinn(language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tz6m84GbMo1n"
   },
   "outputs": [],
   "source": [
    "dataset['afinn_score'] = dataset['content'].apply(afinn.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W50mCwvGNTqY"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4vN0_XS0ncg"
   },
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEKX5T0-0pyP"
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rTmMp6eWD7E"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZukqRsA4BK3"
   },
   "outputs": [],
   "source": [
    "def vaderize(df, textfield):\n",
    "    '''Compute the Vader polarity scores for a textfield.\n",
    "    Returns scores and original dataframe.'''\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    print('Estimating polarity scores for %d cases.' % len(df))\n",
    "    sentiment = df[textfield].apply(analyzer.polarity_scores)\n",
    "\n",
    "    # convert to dataframe\n",
    "    sdf = pd.DataFrame(sentiment.tolist()).add_prefix('vader_score_')\n",
    "\n",
    "    # merge dataframes\n",
    "    df_combined = pd.concat([df, sdf], axis=1)\n",
    "    del df_combined['vader_score_neg']\n",
    "    del df_combined['vader_score_neu']\n",
    "    del df_combined['vader_score_pos']\n",
    "\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfD_l1SM4COZ"
   },
   "outputs": [],
   "source": [
    "dataset=vaderize(dataset,'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFWjHauC49Eo"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqdikIVn-8dl"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset.to_csv('Export_dataset5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n2M0BirNcw8"
   },
   "source": [
    "# **Readability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isKyZ3zJgu2h"
   },
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "#Flesch Reading Ease Index\n",
    "FREI=list()\n",
    "\n",
    "#Flesch-Kionciad Grade Level\n",
    "FKGL=list()\n",
    "\n",
    "#Automated Readability Index\n",
    "ARI=list()\n",
    "\n",
    "#Gunning Fox Index\n",
    "GFI=list()\n",
    "\n",
    "#Coleman-Liau Index\n",
    "CLI=list()\n",
    "\n",
    "for row in range(0,len(dataset)):\n",
    "  FREI.insert(row,(textstat.flesch_reading_ease(dataset['content'][row])))\n",
    "  FKGL.insert(row,(textstat.flesch_kincaid_grade(dataset['content'][row])))\n",
    "  ARI.insert(row,(textstat.automated_readability_index(dataset['content'][row])))\n",
    "  GFI.insert(row,(textstat.gunning_fog(dataset['content'][row])))\n",
    "  CLI.insert(row,(textstat.coleman_liau_index(dataset['content'][row])))\n",
    "\n",
    "dataset['FREI'] = FREI\n",
    "dataset['FKGL'] = FKGL\n",
    "dataset['ARI'] = ARI\n",
    "dataset['GFI'] = GFI\n",
    "dataset['CLI'] = CLI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKyc5aiOnx4p"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset.to_csv('Export_dataset6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "thesis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
