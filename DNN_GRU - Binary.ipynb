{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ad2f69",
   "metadata": {
    "id": "c0ad506d"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import textblob, string\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import layers, models, optimizers, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca69f9f1",
   "metadata": {
    "id": "In_zYnx7iyjm"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv('L:/Thesis/Dateset/FeaturesExtraction_Compeleted - Binary01.csv', encoding='latin-1')\n",
    "\n",
    "df = df.astype(dtype={'title': 'string','url': 'string',\n",
    "                                'date_published': 'string','author_name': 'string',\n",
    "                                'content': 'string','label': 'string'})\n",
    "\n",
    "cols = ['countOfPosWord', 'countOfNegWord', 'NumberOfChar','NumberOfWords','NumberOfSentences','AvgCharPerWord','AvgWordPerSent','NumberOfUpCase','NumberOfPunctuatuion','Orgtitle_contentSim','TopicModellingSim','afinn_score','vader_score_compound','FREI','FKGL','ARI','GFI','CLI']\n",
    "data = pd.read_csv('L:/Thesis/Dateset/FeaturesExtraction_Compeleted - Binary01.csv',  usecols = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b235f5",
   "metadata": {
    "id": "qPaG74F9VoFX"
   },
   "outputs": [],
   "source": [
    "df['text_features']= df['title'] +' '+ df['content'] +' '+ df['author_name'] +' '+ df['date_published']\n",
    "df['Complete_Data'] = df['text_features'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f258724d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    13011\n",
       "True     12754\n",
       "Name: label, dtype: Int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8279b8d",
   "metadata": {
    "id": "6CXPMvsK2Iif"
   },
   "source": [
    "## **SpellCheker**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ed71c",
   "metadata": {
    "id": "j4HlFVjdHcP7"
   },
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99766e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHWMrBrJbsg1",
    "outputId": "5ee1ab7f-43f4-4c5f-b020-1b71b8646bea"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5597bbf2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUyk98dgFhvi",
    "outputId": "17f1c138-ec57-4b63-f89a-41c482feaa21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for row in range(len(df)):\n",
    "  df['Complete_Data'][row] = re.sub(r\"won\\'t\", \"will not\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"can\\'t\", \"can not\", df['Complete_Data'][row])\n",
    "\n",
    "    # general\n",
    "  df['Complete_Data'][row] = re.sub(r\"n\\'t\", \" not\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'re\", \" are\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'s\", \" is\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'d\", \" would\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'ll\", \" will\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'t\", \" not\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'ve\", \" have\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'m\", \" am\", df['Complete_Data'][row])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f924e99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "592f0f46",
    "outputId": "3d66939c-728a-4d34-ab7b-9ea973ae71c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "X = df['Complete_Data']\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "import re\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "    document= [word for word in document if word not in stop_words]\n",
    "    document = [ps.stem(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "    \n",
    "df['Complete_Data'] = documents\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d436c",
   "metadata": {
    "id": "JhOwiMYbI2-a"
   },
   "source": [
    "**Add Numeric features to text features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed748fb4",
   "metadata": {
    "id": "dSQIHcNHdlML"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "data_scaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "185a2025",
   "metadata": {
    "id": "YHKQuov7dH_h"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "data_scaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8604375c",
   "metadata": {
    "id": "kjR7uLY6dlTp"
   },
   "outputs": [],
   "source": [
    "numerical_data=list()\n",
    "for row in range(0,len(df)):\n",
    "  numerical_data.insert(row,data_scaled[row])\n",
    "df['numerical_features'] = numerical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a04cc8f0",
   "metadata": {
    "id": "4lDuf-IydlZ4"
   },
   "outputs": [],
   "source": [
    "df = df.astype(dtype={'numerical_features': 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8a7fd6c",
   "metadata": {
    "id": "r8h8X_lzly67"
   },
   "outputs": [],
   "source": [
    "df['Complete_Data'] = df['Complete_Data'] + df['numerical_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dd13480",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "bUbudSpF2UR9",
    "outputId": "80139565-ad55-4029-fa11-dc38489b6be0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â everi singl swing seat hous democrat endors medicareforal elect track win elect everi singl one â â unexpect elect loss hous democrat made headlin intern divis parti appeal voter â support progress polici lead member lose seat progress say â rep alexandria ocasio cortez democrat repres part bronx queen said anâ â hous democrat swing district support medicar elect without except â everi singl swing seat hous democrat endors medicareforal elect track win elect everi singl one â everi singl swing seat hous democrat endors elect track win elect everi ð ð ½ singl ð ð ½ one ð ð ½ claim gain traction twitter reader ask us look â gener swing seat competit hous district swing democrat republican control differ organ may rate differ seat competit cook polit report nonpartisan polit analysi organ highli regard commonli cite sourc final nov 2 cook list 23 swing seat held democrat incumb seek elect toss lean democrat 23 two incumb rep matt cartwright pennsylvania peter defazio oregonâ co sponsor provid medicar two â ocasio cortezâ campaign ask forâ evid claim provid longer list hous democrat said competit race list includ rep mike levin kati porter california susan wild pennsylvania ann kirkpatrick arizona co sponsor medicar act 2019 levin porterâ wildâ seat histor republican district 2018 kirkpatrick 2018 district parti cook report rate wild seat like democrat seat solid democrat â approach univers virginia center polit claim sent us 2018 race polit scientist alan abramowitz found support medicar vote loser hous race â ocasio cortez campaign also sent list eight member co sponsor r 1384 competit district lost doesnâ directli address claim fact check â â ocasio cortez said everi hous democrat swing seat support medicar race â ocasio cortez statement true far goe leav mislead impress two 23 democrat incumb run electionâ whose race cook polit report rate toss lean democrat sign co sponsor bill rate statement mostli true â jill terreri ramo 11 17 2020[0.18699187 0.01875    0.0950996  0.07958922 0.07734807 0.19891746\\n 0.04392157 0.04959016 0.1507177  0.84820557 0.6027689  0.70173267\\n 0.997      0.9287101  0.04419067 0.05285659 0.04959279 0.25773525]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Complete_Data'][150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fea06c4",
   "metadata": {
    "id": "UtxsAJb1m-w8"
   },
   "outputs": [],
   "source": [
    "for row in range(0,len(df)):\n",
    "    df['Complete_Data'][row] = re.sub(r'\\[',' ',df['Complete_Data'][row])\n",
    "    df['Complete_Data'][row] = re.sub(r'\\]',' ',df['Complete_Data'][row])\n",
    "    df['Complete_Data'][row] = re.sub(r'\\n',' ',df['Complete_Data'][row])\n",
    "    df['Complete_Data'][row] = re.sub(r'\\s+', ' ', df['Complete_Data'][row], flags=re.I)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925a937",
   "metadata": {
    "id": "HGyMxm6mVkCl"
   },
   "source": [
    "## **Set Max features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb2e43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEzfuJCxnJBD",
    "outputId": "82a43333-5b64-4632-ed26-125fcdab4d36"
   },
   "outputs": [],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b287256d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5cfb51a",
    "outputId": "bafcc972-0e5f-4c91-a834-4f1b70ef24f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:  19\n",
      "max:  3072\n",
      "mean: 419.65\n",
      "mean + 2 *sigma: 918.70\n",
      "918\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "lenght=list()\n",
    "for row in range(0,len(df)):\n",
    "    lenght.insert(row , textstat.lexicon_count(df['Complete_Data'][row]))\n",
    "df['lenght']=lenght\n",
    "\n",
    "print('min: ', min(df['lenght']))\n",
    "print('max: ', max(df['lenght']))\n",
    "\n",
    "print('mean: {:.2f}'.format(np.mean(df['lenght'])))\n",
    "\n",
    "print('mean + 2 *sigma: {:.2f}'.format(np.mean(df['lenght'])+ 2.0 * np.std(df['lenght'])))\n",
    "\n",
    "\n",
    "max_features = int(np.mean(df['lenght'])+ 2.0 * np.std(df['lenght']))\n",
    "print(max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8113fbc0",
   "metadata": {
    "id": "643d9b64"
   },
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "df['label'] = encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a7090c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvGtb5dczm6H",
    "outputId": "60565963-b3bc-4224-9d23-22bffd6e9951"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['False', 'True'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c22cf21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLQunBt9PHnx",
    "outputId": "f8147b9d-4aa7-4a88-d790-5f892e335dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18035 1546 6184\n",
      "18035 1546 6184\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.70\n",
    "validation_ratio = 0.10\n",
    "test_ratio = 0.20\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "# the _junk suffix means that we drop that variable completely\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Complete_Data'], df['label'], test_size=0.3)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, train_size=0.2)\n",
    "\n",
    "import tensorflow as tf\n",
    "train_y = tf.keras.utils.to_categorical(y_train, num_classes=5)\n",
    "valid_y = tf.keras.utils.to_categorical(y_val, num_classes=5)\n",
    "test_y = tf.keras.utils.to_categorical(y_test, num_classes=5)\n",
    "\n",
    "print(len(x_train), len(x_val), len(x_test))\n",
    "print(len(train_y), len(valid_y), len(test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf63bc",
   "metadata": {
    "id": "bzreAg3HIZj2"
   },
   "source": [
    "## **word2vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5372c90a",
   "metadata": {
    "id": "ZXKaGCPvAoVi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Embeddings():\n",
    "    \"\"\"\n",
    "    A class to read the word embedding file and to create the word embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, vector_dimension):\n",
    "        self.path = path \n",
    "        self.vector_dimension = vector_dimension\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_coefs(word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    def get_embedding_index(self):\n",
    "        embeddings_index = dict(self.get_coefs(*o.split(\" \")) for o in open(self.path, errors='ignore'))\n",
    "        return embeddings_index\n",
    "\n",
    "    def create_embedding_matrix(self, tokenizer, vocab_size):\n",
    "        \"\"\"\n",
    "        A method to create the embedding matrix\n",
    "        \"\"\"\n",
    "        model_embed = self.get_embedding_index()\n",
    "\n",
    "        embedding_matrix = np.zeros((vocab_size + 1, self.vector_dimension))\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index > max_features:\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    embedding_matrix[index] = model_embed[word]\n",
    "                except:\n",
    "                    continue\n",
    "        return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8d6a99c",
   "metadata": {
    "id": "mLIeNymoDXjU"
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Complete_Data'])\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c78c5ae2",
   "metadata": {
    "id": "7uSlgsw5DGXs"
   },
   "outputs": [],
   "source": [
    "embedding = Embeddings(\n",
    "  'G:/datasets/glove.840B.300d/glove.840B.300d.txt', \n",
    "  vector_dimension = 300\n",
    ")\n",
    "embedding_matrix = embedding.create_embedding_matrix(tokenizer, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c99c8401",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Wa1tcLp4uLN",
    "outputId": "d818864e-4686-4e8f-aea6-a96f0dc52569"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008437165571013606"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6b8e3c3",
   "metadata": {
    "id": "81b4135f"
   },
   "outputs": [],
   "source": [
    "padding_type='post'\n",
    "trunc_type='post'\n",
    "\n",
    "#train_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(train_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "#valid_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(valid_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "train_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "test_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "valid_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(x_val), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "847611d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_pH6AAZVVYk",
    "outputId": "c371cff1-1f78-4794-c91b-c54f882d38bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18035 6184 1546\n"
     ]
    }
   ],
   "source": [
    "print(len(train_seq_x), len(test_seq_x), len(valid_seq_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147b449",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5ec9662",
    "outputId": "855b8346-b5e3-426e-8913-2999c45d50f1"
   },
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('/content/drive/MyDrive/Datasets/Glove.6B/GoogleNews-vectors-negative300.bin',encoding=\"ISO-8859-1\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float64')\n",
    "    \n",
    "#num_word = 10000\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer(oov_token='UNK', filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True)\n",
    "token.fit_on_texts(df['Complete_Data'])\n",
    "word_index = token.word_index\n",
    "print(len(word_index))\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "601c42a2",
   "metadata": {
    "id": "_r-mLVX3-3xY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wintest\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass classes=[0 1], y=0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "25760    1\n",
      "25761    1\n",
      "25762    1\n",
      "25763    1\n",
      "25764    1\n",
      "Name: label, Length: 25765, dtype: int32 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "def compute_classweights(target):\n",
    "    \"\"\"\n",
    "    Computes the weights of the target values based on the samples\n",
    "    :param target: Y-target variable\n",
    "    :return: dictionary object\n",
    "    \"\"\"\n",
    "    # compute class weights\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                     np.unique(target),\n",
    "                                                     target)\n",
    "    \n",
    "    # make the class weight list into dictionary\n",
    "    weights = {}\n",
    "    \n",
    "    # enumerate the list\n",
    "    for index, weight in enumerate(class_weights):\n",
    "        weights[index] = weight\n",
    "        \n",
    "    return weights\n",
    "\n",
    "# Get the class weights for the target variable\n",
    "weights = compute_classweights(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad0f1af0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RThPs-YZE_W6",
    "outputId": "becfccd2-a57b-40c1-ce38-57907ed7395e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.9901237414495427, 1: 1.0100752705033715}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caf30a73",
   "metadata": {
    "id": "bd7aec1e"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "551f9528",
   "metadata": {
    "id": "ODi0zfyvxI7o"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "def scheduler(epoch, lr):\n",
    "    \n",
    "    if epoch < 7:\n",
    "        print('-- lr =%f --',lr) \n",
    "        return lr\n",
    "\n",
    "    elif (epoch%7 == 0):\n",
    "        lr = lr * tensorflow.math.exp(-0.2)\n",
    "        print('** lr =%f **',lr)  \n",
    "        return lr\n",
    "        \n",
    "    else:\n",
    "        print('__ lr =%f __',lr) \n",
    "        return lr\n",
    "    \n",
    "    \n",
    "callback = tensorflow.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32068005",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k56YXy1r_Byo",
    "outputId": "88c27eeb-2cb2-43b7-dc54-9b922ab88a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 918, 300)          31396800  \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 918, 300)          1200      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 918, 256)          330240    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 918, 128)          32896     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 31,771,505\n",
      "Trainable params: 374,105\n",
      "Non-trainable params: 31,397,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import losses\n",
    "\n",
    "\n",
    "embedding_dim =300\n",
    "model_GRU = Sequential()\n",
    "model_GRU.add(layers.Embedding(vocab_size, embedding_dim, input_length=max_features, weights=[embedding_matrix], trainable=False))\n",
    "model_GRU.add(layers.BatchNormalization())\n",
    "model_GRU.add(layers.Bidirectional(layers.GRU(128,return_sequences=True)))\n",
    "model_GRU.add(layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(l2=0.002)))\n",
    "model_GRU.add(layers.GlobalMaxPool1D()) # Remove flatten layer\n",
    "model_GRU.add(layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(l2=0.002)))\n",
    "model_GRU.add(layers.Dropout(0.3))\n",
    "model_GRU.add(layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(l2=0.002)))\n",
    "model_GRU.add(layers.Dropout(0.3))\n",
    "model_GRU.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_GRU.summary()\n",
    "\n",
    "model_GRU.compile(optimizer=tf.keras.optimizers.Adam(8e-6), \n",
    "                  loss=losses.BinaryCrossentropy(), \n",
    "                  metrics=[tf.metrics.AUC(name='accuracy')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2da7fdd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OpFD4zz_HBf",
    "outputId": "4930ef21-07c0-4856-97c2-4e722de177ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 18035 samples, validate on 1546 samples\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 1/50\n",
      "18035/18035 [==============================] - 479s 27ms/sample - loss: 1.2905 - accuracy: 0.6430 - val_loss: 0.9836 - val_accuracy: 0.9252\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 2/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.9593 - accuracy: 0.8917 - val_loss: 0.8429 - val_accuracy: 0.9453\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 3/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.8540 - accuracy: 0.9297 - val_loss: 0.7865 - val_accuracy: 0.9539\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 4/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.7979 - accuracy: 0.9440 - val_loss: 0.7495 - val_accuracy: 0.9597\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 5/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.7581 - accuracy: 0.9530 - val_loss: 0.7169 - val_accuracy: 0.9649\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 6/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.7215 - accuracy: 0.9609 - val_loss: 0.6921 - val_accuracy: 0.9684\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 7/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.6931 - accuracy: 0.9659 - val_loss: 0.6692 - val_accuracy: 0.9710\n",
      "** lr =%f ** tf.Tensor(6.549846e-06, shape=(), dtype=float32)\n",
      "Epoch 8/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.6672 - accuracy: 0.9702 - val_loss: 0.6535 - val_accuracy: 0.9725\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 9/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.6455 - accuracy: 0.9736 - val_loss: 0.6373 - val_accuracy: 0.9741\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 10/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.6286 - accuracy: 0.9758 - val_loss: 0.6287 - val_accuracy: 0.9747\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 11/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.6132 - accuracy: 0.9775 - val_loss: 0.6145 - val_accuracy: 0.9758\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 12/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.5980 - accuracy: 0.9792 - val_loss: 0.6041 - val_accuracy: 0.9763\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 13/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.5838 - accuracy: 0.9809 - val_loss: 0.5936 - val_accuracy: 0.9773\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 14/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.5684 - accuracy: 0.9828 - val_loss: 0.5847 - val_accuracy: 0.9777\n",
      "** lr =%f ** tf.Tensor(5.3625604e-06, shape=(), dtype=float32)\n",
      "Epoch 15/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.5587 - accuracy: 0.9835 - val_loss: 0.5784 - val_accuracy: 0.9780\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 16/50\n",
      "18035/18035 [==============================] - 469s 26ms/sample - loss: 0.5461 - accuracy: 0.9850 - val_loss: 0.5710 - val_accuracy: 0.9785\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 17/50\n",
      "18035/18035 [==============================] - 468s 26ms/sample - loss: 0.5386 - accuracy: 0.9854 - val_loss: 0.5661 - val_accuracy: 0.9786\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 18/50\n",
      "18035/18035 [==============================] - 469s 26ms/sample - loss: 0.5265 - accuracy: 0.9868 - val_loss: 0.5601 - val_accuracy: 0.9790\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 19/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.5162 - accuracy: 0.9878 - val_loss: 0.5531 - val_accuracy: 0.9797\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 20/50\n",
      "18035/18035 [==============================] - 471s 26ms/sample - loss: 0.5077 - accuracy: 0.9884 - val_loss: 0.5487 - val_accuracy: 0.9796\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 21/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.5002 - accuracy: 0.9889 - val_loss: 0.5426 - val_accuracy: 0.9800\n",
      "** lr =%f ** tf.Tensor(4.3904934e-06, shape=(), dtype=float32)\n",
      "Epoch 22/50\n",
      "18035/18035 [==============================] - 469s 26ms/sample - loss: 0.4897 - accuracy: 0.9901 - val_loss: 0.5393 - val_accuracy: 0.9800\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 23/50\n",
      "18035/18035 [==============================] - 469s 26ms/sample - loss: 0.4804 - accuracy: 0.9910 - val_loss: 0.5370 - val_accuracy: 0.9802\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 24/50\n",
      "18035/18035 [==============================] - 469s 26ms/sample - loss: 0.4729 - accuracy: 0.9917 - val_loss: 0.5327 - val_accuracy: 0.9804\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 25/50\n",
      "18035/18035 [==============================] - 469s 26ms/sample - loss: 0.4651 - accuracy: 0.9922 - val_loss: 0.5295 - val_accuracy: 0.9803\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 26/50\n",
      "18035/18035 [==============================] - 469s 26ms/sample - loss: 0.4611 - accuracy: 0.9923 - val_loss: 0.5253 - val_accuracy: 0.9806\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 27/50\n",
      "18035/18035 [==============================] - 472s 26ms/sample - loss: 0.4523 - accuracy: 0.9931 - val_loss: 0.5247 - val_accuracy: 0.9805\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 28/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.4487 - accuracy: 0.9931 - val_loss: 0.5201 - val_accuracy: 0.9808\n",
      "** lr =%f ** tf.Tensor(3.594632e-06, shape=(), dtype=float32)\n",
      "Epoch 29/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.4420 - accuracy: 0.9936 - val_loss: 0.5174 - val_accuracy: 0.9807\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 30/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.4358 - accuracy: 0.9942 - val_loss: 0.5155 - val_accuracy: 0.9807\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 31/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.4307 - accuracy: 0.9947 - val_loss: 0.5135 - val_accuracy: 0.9810\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 32/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.4262 - accuracy: 0.9947 - val_loss: 0.5103 - val_accuracy: 0.9810\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 33/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.4195 - accuracy: 0.9954 - val_loss: 0.5097 - val_accuracy: 0.9812\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 34/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.4136 - accuracy: 0.9958 - val_loss: 0.5073 - val_accuracy: 0.9813\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 35/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.4096 - accuracy: 0.9961 - val_loss: 0.5060 - val_accuracy: 0.9810\n",
      "** lr =%f ** tf.Tensor(2.943036e-06, shape=(), dtype=float32)\n",
      "Epoch 36/50\n",
      "18035/18035 [==============================] - 471s 26ms/sample - loss: 0.4061 - accuracy: 0.9961 - val_loss: 0.5050 - val_accuracy: 0.9812\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 37/50\n",
      "18035/18035 [==============================] - 472s 26ms/sample - loss: 0.4007 - accuracy: 0.9964 - val_loss: 0.5029 - val_accuracy: 0.9813\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 38/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.3995 - accuracy: 0.9963 - val_loss: 0.5012 - val_accuracy: 0.9813\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 39/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.3944 - accuracy: 0.9966 - val_loss: 0.4988 - val_accuracy: 0.9814\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 40/50\n",
      "18035/18035 [==============================] - 471s 26ms/sample - loss: 0.3879 - accuracy: 0.9973 - val_loss: 0.4989 - val_accuracy: 0.9814\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 41/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.3827 - accuracy: 0.9974 - val_loss: 0.4992 - val_accuracy: 0.9813\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.3825 - accuracy: 0.9972 - val_loss: 0.4971 - val_accuracy: 0.9811\n",
      "** lr =%f ** tf.Tensor(2.409554e-06, shape=(), dtype=float32)\n",
      "Epoch 43/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.3779 - accuracy: 0.9975 - val_loss: 0.4972 - val_accuracy: 0.9811\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 44/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.3762 - accuracy: 0.9976 - val_loss: 0.4958 - val_accuracy: 0.9816\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 45/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.3729 - accuracy: 0.9978 - val_loss: 0.4942 - val_accuracy: 0.9811\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 46/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.3700 - accuracy: 0.9977 - val_loss: 0.4935 - val_accuracy: 0.9811\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 47/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.3677 - accuracy: 0.9979 - val_loss: 0.4919 - val_accuracy: 0.9812\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 48/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.3633 - accuracy: 0.9982 - val_loss: 0.4911 - val_accuracy: 0.9813\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 49/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.3620 - accuracy: 0.9982 - val_loss: 0.4902 - val_accuracy: 0.9812\n",
      "** lr =%f ** tf.Tensor(1.972776e-06, shape=(), dtype=float32)\n",
      "Epoch 50/50\n",
      "18035/18035 [==============================] - 470s 26ms/sample - loss: 0.3565 - accuracy: 0.9984 - val_loss: 0.4897 - val_accuracy: 0.9809\n"
     ]
    }
   ],
   "source": [
    "history = model_GRU.fit(train_seq_x, \n",
    "                    y_train,\n",
    "                    batch_size=4,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_data=(valid_seq_x, y_val),\n",
    "                    class_weight=weights,\n",
    "                    callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c45c9d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9993\n",
      "Testing Accuracy:  0.9809\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss, accuracy = model_GRU.evaluate(train_seq_x, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_GRU.evaluate(valid_seq_x, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a158a66f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "id": "ZIomXlxkB4mz",
    "outputId": "e440e457-24e6-48ff-c1cd-9df33970a743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6184/6184 [==============================] - 69s 11ms/sample - loss: 0.4864 - accuracy: 0.9818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4864088183541193, 0.9817758]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GRU.evaluate(test_seq_x,y_test,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4257af75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sSi5cYCC2EZ",
    "outputId": "e3142f7f-0fc7-425b-cf07-74cbaa5171fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_GRU = model_GRU.predict_classes(test_seq_x)\n",
    "pred_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbbdad43",
   "metadata": {
    "id": "bB4BNYvUC-oh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_eval_GRU = y_test.to_numpy()\n",
    "y_eval_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8e8fbbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmRXJIeBDFNJ",
    "outputId": "5da7fed3-0ee5-4d0f-c904-3f61ef3a9ee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      3101\n",
      "           1       0.93      0.92      0.92      3083\n",
      "\n",
      "    accuracy                           0.93      6184\n",
      "   macro avg       0.93      0.93      0.93      6184\n",
      "weighted avg       0.93      0.93      0.93      6184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "matrix1 = classification_report(y_eval_GRU,pred_GRU)\n",
    "print(matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5de8fac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2902  199]\n",
      " [ 262 2821]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_gru = confusion_matrix(y_eval_GRU,pred_GRU)\n",
    "print(confusion_matrix_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4107de6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DNN-LSTM01C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
