{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0051aa14",
   "metadata": {
    "id": "c0ad506d"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import textblob, string\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import layers, models, optimizers, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ef4d85",
   "metadata": {
    "id": "In_zYnx7iyjm"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv('L:/Thesis/Dateset/FeaturesExtraction_Compeleted - Binary01.csv', encoding='latin-1')\n",
    "\n",
    "df = df.astype(dtype={'title': 'string','url': 'string',\n",
    "                                'date_published': 'string','author_name': 'string',\n",
    "                                'content': 'string','label': 'string'})\n",
    "\n",
    "cols = ['countOfPosWord', 'countOfNegWord', 'NumberOfChar','NumberOfWords','NumberOfSentences','AvgCharPerWord','AvgWordPerSent','NumberOfUpCase','NumberOfPunctuatuion','Orgtitle_contentSim','TopicModellingSim','afinn_score','vader_score_compound','FREI','FKGL','ARI','GFI','CLI']\n",
    "data = pd.read_csv('L:/Thesis/Dateset/FeaturesExtraction_Compeleted - Binary01.csv',  usecols = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ef7029",
   "metadata": {
    "id": "qPaG74F9VoFX"
   },
   "outputs": [],
   "source": [
    "df['text_features']= df['title'] +' '+ df['content'] +' '+ df['author_name'] +' '+ df['date_published']\n",
    "df['Complete_Data'] = df['text_features'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c8e154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    13011\n",
       "True     12754\n",
       "Name: label, dtype: Int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc6ecff",
   "metadata": {
    "id": "6CXPMvsK2Iif"
   },
   "source": [
    "## **SpellCheker**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c9679",
   "metadata": {
    "id": "j4HlFVjdHcP7"
   },
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb35522",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHWMrBrJbsg1",
    "outputId": "5ee1ab7f-43f4-4c5f-b020-1b71b8646bea"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e80010f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUyk98dgFhvi",
    "outputId": "17f1c138-ec57-4b63-f89a-41c482feaa21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for row in range(len(df)):\n",
    "  df['Complete_Data'][row] = re.sub(r\"won\\'t\", \"will not\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"can\\'t\", \"can not\", df['Complete_Data'][row])\n",
    "\n",
    "    # general\n",
    "  df['Complete_Data'][row] = re.sub(r\"n\\'t\", \" not\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'re\", \" are\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'s\", \" is\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'d\", \" would\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'ll\", \" will\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'t\", \" not\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'ve\", \" have\", df['Complete_Data'][row])\n",
    "  df['Complete_Data'][row] = re.sub(r\"\\'m\", \" am\", df['Complete_Data'][row])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68f94d98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "592f0f46",
    "outputId": "3d66939c-728a-4d34-ab7b-9ea973ae71c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "X = df['Complete_Data']\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "import re\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "    document= [word for word in document if word not in stop_words]\n",
    "    document = [ps.stem(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "    \n",
    "df['Complete_Data'] = documents\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d48167d",
   "metadata": {
    "id": "JhOwiMYbI2-a"
   },
   "source": [
    "**Add Numeric features to text features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9225b8df",
   "metadata": {
    "id": "dSQIHcNHdlML"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "data_scaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eefccd44",
   "metadata": {
    "id": "YHKQuov7dH_h"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "data_scaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd1d4dcc",
   "metadata": {
    "id": "kjR7uLY6dlTp"
   },
   "outputs": [],
   "source": [
    "numerical_data=list()\n",
    "for row in range(0,len(df)):\n",
    "  numerical_data.insert(row,data_scaled[row])\n",
    "df['numerical_features'] = numerical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f351293",
   "metadata": {
    "id": "4lDuf-IydlZ4"
   },
   "outputs": [],
   "source": [
    "df = df.astype(dtype={'numerical_features': 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a818f9c2",
   "metadata": {
    "id": "r8h8X_lzly67"
   },
   "outputs": [],
   "source": [
    "df['Complete_Data'] = df['Complete_Data'] + df['numerical_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f1648d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "bUbudSpF2UR9",
    "outputId": "80139565-ad55-4029-fa11-dc38489b6be0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â everi singl swing seat hous democrat endors medicareforal elect track win elect everi singl one â â unexpect elect loss hous democrat made headlin intern divis parti appeal voter â support progress polici lead member lose seat progress say â rep alexandria ocasio cortez democrat repres part bronx queen said anâ â hous democrat swing district support medicar elect without except â everi singl swing seat hous democrat endors medicareforal elect track win elect everi singl one â everi singl swing seat hous democrat endors elect track win elect everi ð ð ½ singl ð ð ½ one ð ð ½ claim gain traction twitter reader ask us look â gener swing seat competit hous district swing democrat republican control differ organ may rate differ seat competit cook polit report nonpartisan polit analysi organ highli regard commonli cite sourc final nov 2 cook list 23 swing seat held democrat incumb seek elect toss lean democrat 23 two incumb rep matt cartwright pennsylvania peter defazio oregonâ co sponsor provid medicar two â ocasio cortezâ campaign ask forâ evid claim provid longer list hous democrat said competit race list includ rep mike levin kati porter california susan wild pennsylvania ann kirkpatrick arizona co sponsor medicar act 2019 levin porterâ wildâ seat histor republican district 2018 kirkpatrick 2018 district parti cook report rate wild seat like democrat seat solid democrat â approach univers virginia center polit claim sent us 2018 race polit scientist alan abramowitz found support medicar vote loser hous race â ocasio cortez campaign also sent list eight member co sponsor r 1384 competit district lost doesnâ directli address claim fact check â â ocasio cortez said everi hous democrat swing seat support medicar race â ocasio cortez statement true far goe leav mislead impress two 23 democrat incumb run electionâ whose race cook polit report rate toss lean democrat sign co sponsor bill rate statement mostli true â jill terreri ramo 11 17 2020[0.18699187 0.01875    0.0950996  0.07958922 0.07734807 0.19891746\\n 0.04392157 0.04959016 0.1507177  0.84820557 0.6027689  0.70173267\\n 0.997      0.9287101  0.04419067 0.05285659 0.04959279 0.25773525]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Complete_Data'][150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e9951e",
   "metadata": {
    "id": "UtxsAJb1m-w8"
   },
   "outputs": [],
   "source": [
    "for row in range(0,len(df)):\n",
    "    df['Complete_Data'][row] = re.sub(r'\\[',' ',df['Complete_Data'][row])\n",
    "    df['Complete_Data'][row] = re.sub(r'\\]',' ',df['Complete_Data'][row])\n",
    "    df['Complete_Data'][row] = re.sub(r'\\n',' ',df['Complete_Data'][row])\n",
    "    df['Complete_Data'][row] = re.sub(r'\\s+', ' ', df['Complete_Data'][row], flags=re.I)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663d5e0",
   "metadata": {
    "id": "HGyMxm6mVkCl"
   },
   "source": [
    "## **Set Max features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad19cb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEzfuJCxnJBD",
    "outputId": "82a43333-5b64-4632-ed26-125fcdab4d36"
   },
   "outputs": [],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d04881ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5cfb51a",
    "outputId": "bafcc972-0e5f-4c91-a834-4f1b70ef24f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:  19\n",
      "max:  3072\n",
      "mean: 419.65\n",
      "mean + 2 *sigma: 918.70\n",
      "918\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "lenght=list()\n",
    "for row in range(0,len(df)):\n",
    "    lenght.insert(row , textstat.lexicon_count(df['Complete_Data'][row]))\n",
    "df['lenght']=lenght\n",
    "\n",
    "print('min: ', min(df['lenght']))\n",
    "print('max: ', max(df['lenght']))\n",
    "\n",
    "print('mean: {:.2f}'.format(np.mean(df['lenght'])))\n",
    "\n",
    "print('mean + 2 *sigma: {:.2f}'.format(np.mean(df['lenght'])+ 2.0 * np.std(df['lenght'])))\n",
    "\n",
    "\n",
    "max_features = int(np.mean(df['lenght'])+ 2.0 * np.std(df['lenght']))\n",
    "print(max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e4dfc5e",
   "metadata": {
    "id": "643d9b64"
   },
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "df['label'] = encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2661921",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvGtb5dczm6H",
    "outputId": "60565963-b3bc-4224-9d23-22bffd6e9951"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['False', 'True'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ef08bd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLQunBt9PHnx",
    "outputId": "f8147b9d-4aa7-4a88-d790-5f892e335dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18035 1546 6184\n",
      "18035 1546 6184\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.70\n",
    "validation_ratio = 0.10\n",
    "test_ratio = 0.20\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "# the _junk suffix means that we drop that variable completely\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Complete_Data'], df['label'], test_size=0.3)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, train_size=0.2)\n",
    "\n",
    "#import tensorflow as tf\n",
    "#train_y = tf.keras.utils.to_categorical(y_train, num_classes=5)\n",
    "#valid_y = tf.keras.utils.to_categorical(y_val, num_classes=5)\n",
    "#test_y = tf.keras.utils.to_categorical(y_test, num_classes=5)\n",
    "\n",
    "print(len(x_train), len(x_val), len(x_test))\n",
    "print(len(y_train), len(y_val), len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca57c42",
   "metadata": {
    "id": "bzreAg3HIZj2"
   },
   "source": [
    "## **word2vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a16c635d",
   "metadata": {
    "id": "ZXKaGCPvAoVi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Embeddings():\n",
    "    \"\"\"\n",
    "    A class to read the word embedding file and to create the word embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, vector_dimension):\n",
    "        self.path = path \n",
    "        self.vector_dimension = vector_dimension\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_coefs(word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    def get_embedding_index(self):\n",
    "        embeddings_index = dict(self.get_coefs(*o.split(\" \")) for o in open(self.path, errors='ignore'))\n",
    "        return embeddings_index\n",
    "\n",
    "    def create_embedding_matrix(self, tokenizer, vocab_size):\n",
    "        \"\"\"\n",
    "        A method to create the embedding matrix\n",
    "        \"\"\"\n",
    "        model_embed = self.get_embedding_index()\n",
    "\n",
    "        embedding_matrix = np.zeros((vocab_size + 1, self.vector_dimension))\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index > max_features:\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    embedding_matrix[index] = model_embed[word]\n",
    "                except:\n",
    "                    continue\n",
    "        return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c668b02",
   "metadata": {
    "id": "mLIeNymoDXjU"
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Complete_Data'])\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eba1553",
   "metadata": {
    "id": "7uSlgsw5DGXs"
   },
   "outputs": [],
   "source": [
    "embedding = Embeddings(\n",
    "  'G:/datasets/glove.840B.300d/glove.840B.300d.txt', \n",
    "  vector_dimension = 300\n",
    ")\n",
    "embedding_matrix = embedding.create_embedding_matrix(tokenizer, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cd903eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Wa1tcLp4uLN",
    "outputId": "d818864e-4686-4e8f-aea6-a96f0dc52569"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008437165571013606"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e9ee378",
   "metadata": {
    "id": "81b4135f"
   },
   "outputs": [],
   "source": [
    "padding_type='post'\n",
    "trunc_type='post'\n",
    "\n",
    "#train_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(train_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "#valid_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(valid_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "train_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "test_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "valid_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(x_val), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27268f11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_pH6AAZVVYk",
    "outputId": "c371cff1-1f78-4794-c91b-c54f882d38bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18035 6184 1546\n"
     ]
    }
   ],
   "source": [
    "print(len(train_seq_x), len(test_seq_x), len(valid_seq_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edccbb8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5ec9662",
    "outputId": "855b8346-b5e3-426e-8913-2999c45d50f1"
   },
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('/content/drive/MyDrive/Datasets/Glove.6B/GoogleNews-vectors-negative300.bin',encoding=\"ISO-8859-1\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float64')\n",
    "    \n",
    "#num_word = 10000\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer(oov_token='UNK', filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True)\n",
    "token.fit_on_texts(df['Complete_Data'])\n",
    "word_index = token.word_index\n",
    "print(len(word_index))\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=max_features, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e05a1437",
   "metadata": {
    "id": "_r-mLVX3-3xY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wintest\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass classes=[0 1], y=0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "25760    1\n",
      "25761    1\n",
      "25762    1\n",
      "25763    1\n",
      "25764    1\n",
      "Name: label, Length: 25765, dtype: int32 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "def compute_classweights(target):\n",
    "    \"\"\"\n",
    "    Computes the weights of the target values based on the samples\n",
    "    :param target: Y-target variable\n",
    "    :return: dictionary object\n",
    "    \"\"\"\n",
    "    # compute class weights\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                     np.unique(target),\n",
    "                                                     target)\n",
    "    \n",
    "    # make the class weight list into dictionary\n",
    "    weights = {}\n",
    "    \n",
    "    # enumerate the list\n",
    "    for index, weight in enumerate(class_weights):\n",
    "        weights[index] = weight\n",
    "        \n",
    "    return weights\n",
    "\n",
    "# Get the class weights for the target variable\n",
    "weights = compute_classweights(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc2ed421",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RThPs-YZE_W6",
    "outputId": "becfccd2-a57b-40c1-ce38-57907ed7395e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.9901237414495427, 1: 1.0100752705033715}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eed1aa76",
   "metadata": {
    "id": "bd7aec1e"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0f3d32d",
   "metadata": {
    "id": "ODi0zfyvxI7o"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "def scheduler(epoch, lr):\n",
    "    \n",
    "    if epoch < 7:\n",
    "        print('-- lr =%f --',lr) \n",
    "        return lr\n",
    "\n",
    "    elif (epoch%7 == 0):\n",
    "        lr = lr * tensorflow.math.exp(-0.2)\n",
    "        print('** lr =%f **',lr)  \n",
    "        return lr\n",
    "        \n",
    "    else:\n",
    "        print('__ lr =%f __',lr) \n",
    "        return lr\n",
    "    \n",
    "    \n",
    "callback = tensorflow.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbccd162",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8gAPIw2MFI_F",
    "outputId": "3269133c-0229-4d2f-ee13-be211447e9e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 918, 300)          31396800  \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 918, 300)          1200      \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 918, 256)          439296    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 918, 128)          32896     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 31,880,561\n",
      "Trainable params: 483,161\n",
      "Non-trainable params: 31,397,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "embedding_dim =300\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=max_features, weights=[embedding_matrix], trainable=False))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Bidirectional(layers.LSTM(128,return_sequences=True)))\n",
    "model.add(layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(l2=0.002)))\n",
    "model.add(layers.GlobalMaxPool1D()) # Remove flatten layer\n",
    "model.add(layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(l2=0.002)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(l2=0.002)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9663b47",
   "metadata": {
    "id": "LIbtZlYUF5Tc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(8e-6), \n",
    "                  loss=losses.BinaryCrossentropy(), \n",
    "                  metrics=[tf.metrics.AUC(name='accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65eea5d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebqqFgdRGMmi",
    "outputId": "b5491f8f-209c-4039-bec1-bbf1f333ad86",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 18035 samples, validate on 1546 samples\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 1/50\n",
      "18035/18035 [==============================] - 524s 29ms/sample - loss: 1.1609 - accuracy: 0.7551 - val_loss: 0.9374 - val_accuracy: 0.9488\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 2/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.9075 - accuracy: 0.9181 - val_loss: 0.7763 - val_accuracy: 0.9639\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 3/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.7902 - accuracy: 0.9478 - val_loss: 0.6978 - val_accuracy: 0.9738\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 4/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.7214 - accuracy: 0.9607 - val_loss: 0.6490 - val_accuracy: 0.9780\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 5/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.6723 - accuracy: 0.9675 - val_loss: 0.6160 - val_accuracy: 0.9798\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 6/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.6326 - accuracy: 0.9726 - val_loss: 0.5897 - val_accuracy: 0.9810\n",
      "-- lr =%f -- 7.999999979801942e-06\n",
      "Epoch 7/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.6044 - accuracy: 0.9753 - val_loss: 0.5669 - val_accuracy: 0.9820\n",
      "** lr =%f ** tf.Tensor(6.549846e-06, shape=(), dtype=float32)\n",
      "Epoch 8/50\n",
      "18035/18035 [==============================] - 510s 28ms/sample - loss: 0.5779 - accuracy: 0.9784 - val_loss: 0.5513 - val_accuracy: 0.9825\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 9/50\n",
      "18035/18035 [==============================] - 508s 28ms/sample - loss: 0.5580 - accuracy: 0.9803 - val_loss: 0.5378 - val_accuracy: 0.9829\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 10/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.5398 - accuracy: 0.9819 - val_loss: 0.5249 - val_accuracy: 0.9833\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 11/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.5198 - accuracy: 0.9842 - val_loss: 0.5154 - val_accuracy: 0.9838\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 12/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.5027 - accuracy: 0.9857 - val_loss: 0.5037 - val_accuracy: 0.9840\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 13/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.4903 - accuracy: 0.9864 - val_loss: 0.4937 - val_accuracy: 0.9843\n",
      "__ lr =%f __ 6.549846148118377e-06\n",
      "Epoch 14/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.4751 - accuracy: 0.9877 - val_loss: 0.4887 - val_accuracy: 0.9845\n",
      "** lr =%f ** tf.Tensor(5.3625604e-06, shape=(), dtype=float32)\n",
      "Epoch 15/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.4598 - accuracy: 0.9892 - val_loss: 0.4776 - val_accuracy: 0.9844\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 16/50\n",
      "18035/18035 [==============================] - 507s 28ms/sample - loss: 0.4495 - accuracy: 0.9898 - val_loss: 0.4715 - val_accuracy: 0.9845\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 17/50\n",
      "18035/18035 [==============================] - 507s 28ms/sample - loss: 0.4391 - accuracy: 0.9907 - val_loss: 0.4660 - val_accuracy: 0.9846\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 18/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.4280 - accuracy: 0.9916 - val_loss: 0.4602 - val_accuracy: 0.9848\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 19/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.4174 - accuracy: 0.9924 - val_loss: 0.4549 - val_accuracy: 0.9848\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 20/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.4078 - accuracy: 0.9930 - val_loss: 0.4521 - val_accuracy: 0.9847\n",
      "__ lr =%f __ 5.362560386856785e-06\n",
      "Epoch 21/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.3989 - accuracy: 0.9935 - val_loss: 0.4451 - val_accuracy: 0.9850\n",
      "** lr =%f ** tf.Tensor(4.3904934e-06, shape=(), dtype=float32)\n",
      "Epoch 22/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.3880 - accuracy: 0.9944 - val_loss: 0.4444 - val_accuracy: 0.9849\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 23/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.3820 - accuracy: 0.9946 - val_loss: 0.4388 - val_accuracy: 0.9853\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 24/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.3722 - accuracy: 0.9954 - val_loss: 0.4387 - val_accuracy: 0.9855\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 25/50\n",
      "18035/18035 [==============================] - 509s 28ms/sample - loss: 0.3664 - accuracy: 0.9955 - val_loss: 0.4321 - val_accuracy: 0.9855\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 26/50\n",
      "18035/18035 [==============================] - 510s 28ms/sample - loss: 0.3590 - accuracy: 0.9961 - val_loss: 0.4287 - val_accuracy: 0.9855\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 27/50\n",
      "18035/18035 [==============================] - 510s 28ms/sample - loss: 0.3529 - accuracy: 0.9962 - val_loss: 0.4268 - val_accuracy: 0.9853\n",
      "__ lr =%f __ 4.390493359096581e-06\n",
      "Epoch 28/50\n",
      "18035/18035 [==============================] - 509s 28ms/sample - loss: 0.3462 - accuracy: 0.9967 - val_loss: 0.4278 - val_accuracy: 0.9854\n",
      "** lr =%f ** tf.Tensor(3.594632e-06, shape=(), dtype=float32)\n",
      "Epoch 29/50\n",
      "18035/18035 [==============================] - 510s 28ms/sample - loss: 0.3394 - accuracy: 0.9970 - val_loss: 0.4222 - val_accuracy: 0.9856\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 30/50\n",
      "18035/18035 [==============================] - 509s 28ms/sample - loss: 0.3339 - accuracy: 0.9973 - val_loss: 0.4184 - val_accuracy: 0.9856\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 31/50\n",
      "18035/18035 [==============================] - 507s 28ms/sample - loss: 0.3264 - accuracy: 0.9978 - val_loss: 0.4190 - val_accuracy: 0.9858\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 32/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.3231 - accuracy: 0.9978 - val_loss: 0.4141 - val_accuracy: 0.9857\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 33/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.3191 - accuracy: 0.9979 - val_loss: 0.4154 - val_accuracy: 0.9858\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 34/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.3132 - accuracy: 0.9981 - val_loss: 0.4137 - val_accuracy: 0.9856\n",
      "__ lr =%f __ 3.5946320622315397e-06\n",
      "Epoch 35/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.3105 - accuracy: 0.9981 - val_loss: 0.4114 - val_accuracy: 0.9856\n",
      "** lr =%f ** tf.Tensor(2.943036e-06, shape=(), dtype=float32)\n",
      "Epoch 36/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.3033 - accuracy: 0.9986 - val_loss: 0.4127 - val_accuracy: 0.9853\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 37/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.3003 - accuracy: 0.9986 - val_loss: 0.4085 - val_accuracy: 0.9853\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 38/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.2971 - accuracy: 0.9987 - val_loss: 0.4100 - val_accuracy: 0.9853\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 39/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.2919 - accuracy: 0.9989 - val_loss: 0.4068 - val_accuracy: 0.9852\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 40/50\n",
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.2886 - accuracy: 0.9989 - val_loss: 0.4108 - val_accuracy: 0.9852\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 41/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.2858 - accuracy: 0.9989 - val_loss: 0.4100 - val_accuracy: 0.9851\n",
      "__ lr =%f __ 2.9430359518300975e-06\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18035/18035 [==============================] - 505s 28ms/sample - loss: 0.2829 - accuracy: 0.9989 - val_loss: 0.4015 - val_accuracy: 0.9853\n",
      "** lr =%f ** tf.Tensor(2.409554e-06, shape=(), dtype=float32)\n",
      "Epoch 43/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.2772 - accuracy: 0.9993 - val_loss: 0.4022 - val_accuracy: 0.9853\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 44/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.2757 - accuracy: 0.9992 - val_loss: 0.4047 - val_accuracy: 0.9854\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 45/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.2719 - accuracy: 0.9993 - val_loss: 0.4004 - val_accuracy: 0.9850\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 46/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.2682 - accuracy: 0.9994 - val_loss: 0.4028 - val_accuracy: 0.9853\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 47/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.2663 - accuracy: 0.9994 - val_loss: 0.3994 - val_accuracy: 0.9849\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 48/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.2618 - accuracy: 0.9994 - val_loss: 0.4082 - val_accuracy: 0.9845\n",
      "__ lr =%f __ 2.409554099358502e-06\n",
      "Epoch 49/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.2594 - accuracy: 0.9996 - val_loss: 0.4023 - val_accuracy: 0.9849\n",
      "** lr =%f ** tf.Tensor(1.972776e-06, shape=(), dtype=float32)\n",
      "Epoch 50/50\n",
      "18035/18035 [==============================] - 506s 28ms/sample - loss: 0.2574 - accuracy: 0.9995 - val_loss: 0.3997 - val_accuracy: 0.9849\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_seq_x, \n",
    "                    y_train,\n",
    "                    batch_size=4,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_data=(valid_seq_x, y_val),\n",
    "                    class_weight=weights,\n",
    "                    callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "deb123c0",
   "metadata": {
    "id": "F5uRnPqPJ4EF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9998\n",
      "Testing Accuracy:  0.9849\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(train_seq_x, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(valid_seq_x, y_val, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e5263aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdeDGbvAskcq",
    "outputId": "d2b7220f-13da-4ed3-8e13-8016ebaa4530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6184/6184 [==============================] - 71s 12ms/sample - loss: 0.3853 - accuracy: 0.9860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3852500626371564, 0.9860035]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_seq_x,y_test, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c175a665",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPwL-5MZtfXw",
    "outputId": "17f0b2d7-b248-4ca4-803f-652b1410e1cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_LSTM =model.predict_classes(test_seq_x)\n",
    "pred_LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6d0b5c5",
   "metadata": {
    "id": "YoNkrLtODBM_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_eval = y_test.to_numpy()\n",
    "y_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a8bd5ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfmLcU35tlQw",
    "outputId": "4306b092-0b5a-4f5b-c223-faed0096b34c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93      3193\n",
      "           1       0.93      0.92      0.93      2991\n",
      "\n",
      "    accuracy                           0.93      6184\n",
      "   macro avg       0.93      0.93      0.93      6184\n",
      "weighted avg       0.93      0.93      0.93      6184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "matrix = classification_report(y_eval,pred_LSTM)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22ec3c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3000  193]\n",
      " [ 232 2759]]\n"
     ]
    }
   ],
   "source": [
    "confusionmatrix = confusion_matrix(y_eval,pred_LSTM)\n",
    "print(confusionmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41425c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DNN-LSTM01C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
